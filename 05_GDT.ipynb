{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unita: GDT\n",
    "\n",
    "## Semplice classificazione basata su Logistic Regression e Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in seguito ci servono\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# partiamo dall'output dell'unità 04\n",
    "# folder dove e' posizionato il file pickle\n",
    "pickle_file = \"../datasets/notMNIST.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# carico i dati\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  saved = pickle.load(f)\n",
    "  train_dataset = saved['train_dataset']\n",
    "  train_labels = saved['train_labels']\n",
    "  valid_dataset = saved['valid_dataset']\n",
    "  valid_labels = saved['valid_labels']\n",
    "  test_dataset = saved['test_dataset']\n",
    "  test_labels = saved['test_labels']\n",
    "  del saved  # garbage collector per liberare memoria\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14313726  0.37450981  0.22941177  0.20196079  0.18235295  0.15490197\n",
      "   0.13137256  0.10392157  0.07647059  0.0372549  -0.00196078 -0.03333334\n",
      "  -0.06470589 -0.08431373 -0.10392157 -0.10392157 -0.08039216 -0.05686275\n",
      "  -0.03333334  0.00588235  0.08039216  0.13921569  0.18235295  0.23333333\n",
      "  -0.10784314 -0.48039216 -0.5        -0.5       ]\n",
      " [-0.19803922  0.5        -0.04117647 -0.34705883 -0.27254903 -0.24509804\n",
      "  -0.21372549 -0.18235295 -0.15490197 -0.11568628 -0.07647059 -0.04509804\n",
      "  -0.02156863 -0.00588235  0.00196078  0.00980392 -0.00196078 -0.00980392\n",
      "  -0.0254902  -0.06470589 -0.12745099 -0.19019608  0.35490197  0.5         0.5\n",
      "   0.3392157  -0.04509804 -0.43333334]\n",
      " [-0.28431374  0.5        -0.0254902  -0.5        -0.49215686 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.49607843\n",
      "  -0.5        -0.44509804  0.32745099  0.5         0.48823529  0.48431373\n",
      "   0.5        -0.20980392]\n",
      " [-0.3509804   0.43725491  0.14705883 -0.5        -0.48823529 -0.49215686\n",
      "  -0.48823529 -0.48823529 -0.48823529 -0.48823529 -0.48431373 -0.48431373\n",
      "  -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373\n",
      "  -0.48431373 -0.47254902 -0.5        -0.28823531  0.42941177  0.5\n",
      "   0.48823529  0.5         0.42941177 -0.31568629]\n",
      " [-0.41764706  0.34705883  0.29607844 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.48823529 -0.48431373 -0.48431373 -0.48431373 -0.48431373\n",
      "  -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373\n",
      "  -0.47254902 -0.5        -0.18627451  0.5         0.5         0.48823529\n",
      "   0.5         0.33529413 -0.39411765]\n",
      " [-0.48431373  0.25686276  0.42156863 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.48039216\n",
      "  -0.5        -0.07647059  0.5         0.48431373  0.48431373  0.5\n",
      "   0.2254902  -0.47254902]\n",
      " [-0.5         0.15490197  0.48823529 -0.4254902  -0.5        -0.49607843\n",
      "  -0.5        -0.48039216 -0.16666667 -0.08039216 -0.07647059 -0.05686275\n",
      "  -0.04117647 -0.03333334 -0.0254902  -0.0372549  -0.06078431 -0.08039216\n",
      "  -0.11176471 -0.17058824 -0.30000001  0.11960784  0.5         0.47647059\n",
      "   0.47647059  0.5         0.11960784 -0.5       ]\n",
      " [-0.5         0.06078431  0.5        -0.31568629 -0.5        -0.48823529\n",
      "  -0.5        -0.44901961  0.31568629  0.5         0.49215686  0.5         0.5\n",
      "   0.5         0.5         0.5         0.5         0.5         0.5         0.5\n",
      "   0.5         0.5         0.5         0.5         0.48431373  0.5\n",
      "   0.00980392 -0.5       ]\n",
      " [-0.5        -0.05294118  0.5        -0.19411765 -0.5        -0.48431373\n",
      "  -0.5        -0.47647059  0.21372549  0.5         0.47254902  0.48823529\n",
      "   0.47647059  0.31568629  0.25294119  0.2372549   0.21372549  0.19411765\n",
      "   0.19803922  0.2254902   0.25686276  0.28823531  0.3392157   0.39803922\n",
      "   0.44117647  0.5        -0.11960784 -0.5       ]\n",
      " [-0.5        -0.17058824  0.5        -0.07254902 -0.5        -0.48039216\n",
      "  -0.5        -0.49607843  0.16666667  0.5         0.48431373  0.5\n",
      "   0.46862745 -0.27254903 -0.5        -0.49215686 -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.49607843 -0.46078432 -0.39411765\n",
      "  -0.33529413 -0.44117647 -0.5       ]\n",
      " [-0.5        -0.28431374  0.5         0.04901961 -0.5        -0.47647059\n",
      "  -0.5        -0.5         0.10784314  0.5         0.48431373  0.5\n",
      "   0.48431373 -0.21372549 -0.5        -0.48431373 -0.49215686 -0.49215686\n",
      "  -0.48823529 -0.49215686 -0.49607843 -0.49607843 -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.39019608  0.49607843  0.15490197 -0.5        -0.48431373\n",
      "  -0.49215686 -0.5         0.05686275  0.5         0.47647059  0.5\n",
      "   0.49607843 -0.1627451  -0.5        -0.47254902 -0.48823529 -0.5        -0.5\n",
      "  -0.49215686 -0.48431373 -0.49607843 -0.49607843 -0.49607843 -0.49215686\n",
      "  -0.49215686 -0.49607843 -0.5       ]\n",
      " [-0.5        -0.49607843  0.43333334  0.25294119 -0.5        -0.5\n",
      "  -0.48823529 -0.5         0.00196078  0.5         0.48039216  0.5\n",
      "   0.49607843 -0.1627451  -0.5        -0.48823529 -0.5        -0.5\n",
      "  -0.45294118 -0.46470588 -0.5        -0.49607843 -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5         0.34705883  0.34313726 -0.43333334 -0.5\n",
      "  -0.48039216 -0.5        -0.06470589  0.5         0.47254902  0.5         0.5\n",
      "   0.23333333  0.05294118  0.13137256  0.16666667  0.25294119  0.41764706\n",
      "   0.2647059  -0.18627451 -0.46862745 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5         0.20980392  0.4254902  -0.36666667 -0.5\n",
      "  -0.48823529 -0.5        -0.44117647 -0.33137256 -0.32352942 -0.29607844\n",
      "  -0.2764706  -0.21372549 -0.18627451 -0.20196079 -0.31176472  0.10392157\n",
      "   0.5         0.49215686  0.5         0.36274511 -0.20980392 -0.5\n",
      "  -0.49607843 -0.5        -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5         0.07647059  0.5        -0.30000001 -0.5\n",
      "  -0.48823529 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.48823529 -0.5         0.09607843  0.5\n",
      "   0.48039216  0.47647059  0.5        -0.00588235 -0.5        -0.49607843\n",
      "  -0.5        -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5        -0.05294118  0.5        -0.23333333 -0.5\n",
      "  -0.48431373 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.48823529 -0.5         0.21764706  0.5\n",
      "   0.49215686  0.48823529  0.5        -0.12745099 -0.5        -0.49215686\n",
      "  -0.5        -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5        -0.16666667  0.5        -0.13529412 -0.5\n",
      "  -0.48431373 -0.5        -0.40196079 -0.1509804  -0.14705883 -0.13921569\n",
      "  -0.13137256 -0.13529412 -0.14313726 -0.14313726 -0.20196079  0.3509804\n",
      "   0.5         0.48431373  0.47647059  0.5        -0.20588236 -0.5\n",
      "  -0.48823529 -0.5        -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5        -0.28039217  0.5        -0.01372549 -0.5\n",
      "  -0.48039216 -0.5        -0.21372549  0.5         0.49607843  0.49607843\n",
      "   0.5         0.5         0.5         0.5         0.5         0.5         0.5\n",
      "   0.5         0.48431373  0.5        -0.26078433 -0.5        -0.48823529\n",
      "  -0.5        -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5        -0.38235295  0.5         0.10392157 -0.5\n",
      "  -0.48431373 -0.5        -0.26078433  0.46078432  0.49215686  0.48039216\n",
      "   0.5         0.44117647  0.30392158  0.29215688  0.29215688  0.30000001\n",
      "   0.31176472  0.31960785  0.33137256  0.33137256 -0.3392157  -0.5\n",
      "  -0.49215686 -0.5        -0.5        -0.5       ]\n",
      " [-0.49607843 -0.5        -0.42156863  0.44117647  0.21764706 -0.5\n",
      "  -0.49215686 -0.5        -0.28039217  0.45294118  0.5         0.48039216\n",
      "   0.5         0.2254902  -0.5        -0.49607843 -0.49607843 -0.5        -0.5\n",
      "  -0.49215686 -0.48823529 -0.48431373 -0.49607843 -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.49607843 -0.5        -0.44509804  0.35490197  0.31568629 -0.48823529\n",
      "  -0.49607843 -0.5        -0.30000001  0.43333334  0.5         0.48039216\n",
      "   0.5         0.22156863 -0.5        -0.49607843 -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.49607843 -0.5        -0.46862745  0.28039217  0.40196079 -0.47647059\n",
      "  -0.5        -0.5        -0.32352942  0.4137255   0.5         0.48039216\n",
      "   0.5         0.23333333 -0.5        -0.49607843 -0.49607843 -0.5\n",
      "  -0.49607843 -0.49607843 -0.49607843 -0.49607843 -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5       ]\n",
      " [-0.49607843 -0.5        -0.48431373  0.21372549  0.46862745 -0.45294118\n",
      "  -0.48823529 -0.5        -0.33137256  0.39803922  0.5         0.48039216\n",
      "   0.5         0.25294119 -0.49215686 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.49607843 -0.5        -0.49215686  0.17843138  0.49215686 -0.46470588\n",
      "  -0.5        -0.5        -0.37450981  0.37058824  0.5         0.48039216\n",
      "   0.5         0.2764706  -0.46470588 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.49607843 -0.5        -0.49215686  0.18235295  0.5         0.08039216\n",
      "   0.06862745  0.09607843  0.17450981  0.44509804  0.49607843  0.47254902\n",
      "   0.5         0.29215688 -0.44117647 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.30000001  0.13137256  0.49607843\n",
      "   0.5         0.5         0.5         0.5         0.5         0.48823529\n",
      "   0.5         0.33137256 -0.40980393 -0.5        -0.49215686 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.49607843 -0.5        -0.37450981\n",
      "   0.05686275  0.24117647  0.20196079  0.17843138  0.14705883  0.11176471\n",
      "   0.1        -0.04117647 -0.45294118 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]]\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(train_labels[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Riportiamo i dati nel formato adatto al processamento: matrix 1-dim + vettore 1-hot encoding\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  # -1 indica che la dimensione iniziale rimane invariata\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  # aggiungo una dimensione a labels\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.14313726  0.37450981  0.22941177  0.20196079  0.18235295  0.15490197\n",
      "  0.13137256  0.10392157  0.07647059  0.0372549  -0.00196078 -0.03333334\n",
      " -0.06470589 -0.08431373 -0.10392157 -0.10392157 -0.08039216 -0.05686275\n",
      " -0.03333334  0.00588235  0.08039216  0.13921569  0.18235295  0.23333333\n",
      " -0.10784314 -0.48039216 -0.5        -0.5        -0.19803922  0.5\n",
      " -0.04117647 -0.34705883 -0.27254903 -0.24509804 -0.21372549 -0.18235295\n",
      " -0.15490197 -0.11568628 -0.07647059 -0.04509804 -0.02156863 -0.00588235\n",
      "  0.00196078  0.00980392 -0.00196078 -0.00980392 -0.0254902  -0.06470589\n",
      " -0.12745099 -0.19019608  0.35490197  0.5         0.5         0.3392157\n",
      " -0.04509804 -0.43333334 -0.28431374  0.5        -0.0254902  -0.5\n",
      " -0.49215686 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.49607843 -0.5        -0.44509804  0.32745099  0.5\n",
      "  0.48823529  0.48431373  0.5        -0.20980392 -0.3509804   0.43725491\n",
      "  0.14705883 -0.5        -0.48823529 -0.49215686 -0.48823529 -0.48823529\n",
      " -0.48823529 -0.48823529 -0.48431373 -0.48431373 -0.48431373 -0.48431373\n",
      " -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.47254902\n",
      " -0.5        -0.28823531  0.42941177  0.5         0.48823529  0.5\n",
      "  0.42941177 -0.31568629 -0.41764706  0.34705883  0.29607844 -0.5\n",
      " -0.49607843 -0.5        -0.5        -0.5        -0.48823529 -0.48431373\n",
      " -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373\n",
      " -0.48431373 -0.48431373 -0.48431373 -0.47254902 -0.5        -0.18627451\n",
      "  0.5         0.5         0.48823529  0.5         0.33529413 -0.39411765\n",
      " -0.48431373  0.25686276  0.42156863 -0.5        -0.49607843 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.48039216\n",
      " -0.5        -0.07647059  0.5         0.48431373  0.48431373  0.5\n",
      "  0.2254902  -0.47254902 -0.5         0.15490197  0.48823529 -0.4254902\n",
      " -0.5        -0.49607843 -0.5        -0.48039216 -0.16666667 -0.08039216\n",
      " -0.07647059 -0.05686275 -0.04117647 -0.03333334 -0.0254902  -0.0372549\n",
      " -0.06078431 -0.08039216 -0.11176471 -0.17058824 -0.30000001  0.11960784\n",
      "  0.5         0.47647059  0.47647059  0.5         0.11960784 -0.5        -0.5\n",
      "  0.06078431  0.5        -0.31568629 -0.5        -0.48823529 -0.5\n",
      " -0.44901961  0.31568629  0.5         0.49215686  0.5         0.5         0.5\n",
      "  0.5         0.5         0.5         0.5         0.5         0.5         0.5\n",
      "  0.5         0.5         0.5         0.48431373  0.5         0.00980392\n",
      " -0.5        -0.5        -0.05294118  0.5        -0.19411765 -0.5\n",
      " -0.48431373 -0.5        -0.47647059  0.21372549  0.5         0.47254902\n",
      "  0.48823529  0.47647059  0.31568629  0.25294119  0.2372549   0.21372549\n",
      "  0.19411765  0.19803922  0.2254902   0.25686276  0.28823531  0.3392157\n",
      "  0.39803922  0.44117647  0.5        -0.11960784 -0.5        -0.5\n",
      " -0.17058824  0.5        -0.07254902 -0.5        -0.48039216 -0.5\n",
      " -0.49607843  0.16666667  0.5         0.48431373  0.5         0.46862745\n",
      " -0.27254903 -0.5        -0.49215686 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.49607843 -0.46078432 -0.39411765 -0.33529413\n",
      " -0.44117647 -0.5        -0.5        -0.28431374  0.5         0.04901961\n",
      " -0.5        -0.47647059 -0.5        -0.5         0.10784314  0.5\n",
      "  0.48431373  0.5         0.48431373 -0.21372549 -0.5        -0.48431373\n",
      " -0.49215686 -0.49215686 -0.48823529 -0.49215686 -0.49607843 -0.49607843\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.39019608  0.49607843  0.15490197 -0.5        -0.48431373 -0.49215686\n",
      " -0.5         0.05686275  0.5         0.47647059  0.5         0.49607843\n",
      " -0.1627451  -0.5        -0.47254902 -0.48823529 -0.5        -0.5\n",
      " -0.49215686 -0.48431373 -0.49607843 -0.49607843 -0.49607843 -0.49215686\n",
      " -0.49215686 -0.49607843 -0.5        -0.5        -0.49607843  0.43333334\n",
      "  0.25294119 -0.5        -0.5        -0.48823529 -0.5         0.00196078\n",
      "  0.5         0.48039216  0.5         0.49607843 -0.1627451  -0.5\n",
      " -0.48823529 -0.5        -0.5        -0.45294118 -0.46470588 -0.5\n",
      " -0.49607843 -0.49607843 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.49215686 -0.5         0.34705883  0.34313726 -0.43333334 -0.5\n",
      " -0.48039216 -0.5        -0.06470589  0.5         0.47254902  0.5         0.5\n",
      "  0.23333333  0.05294118  0.13137256  0.16666667  0.25294119  0.41764706\n",
      "  0.2647059  -0.18627451 -0.46862745 -0.5        -0.49607843 -0.5        -0.5\n",
      " -0.5        -0.5        -0.49215686 -0.5         0.20980392  0.4254902\n",
      " -0.36666667 -0.5        -0.48823529 -0.5        -0.44117647 -0.33137256\n",
      " -0.32352942 -0.29607844 -0.2764706  -0.21372549 -0.18627451 -0.20196079\n",
      " -0.31176472  0.10392157  0.5         0.49215686  0.5         0.36274511\n",
      " -0.20980392 -0.5        -0.49607843 -0.5        -0.5        -0.5\n",
      " -0.49215686 -0.5         0.07647059  0.5        -0.30000001 -0.5\n",
      " -0.48823529 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.48823529 -0.5         0.09607843  0.5\n",
      "  0.48039216  0.47647059  0.5        -0.00588235 -0.5        -0.49607843\n",
      " -0.5        -0.5        -0.5        -0.49215686 -0.5        -0.05294118\n",
      "  0.5        -0.23333333 -0.5        -0.48431373 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.48823529\n",
      " -0.5         0.21764706  0.5         0.49215686  0.48823529  0.5\n",
      " -0.12745099 -0.5        -0.49215686 -0.5        -0.5        -0.5\n",
      " -0.49215686 -0.5        -0.16666667  0.5        -0.13529412 -0.5\n",
      " -0.48431373 -0.5        -0.40196079 -0.1509804  -0.14705883 -0.13921569\n",
      " -0.13137256 -0.13529412 -0.14313726 -0.14313726 -0.20196079  0.3509804\n",
      "  0.5         0.48431373  0.47647059  0.5        -0.20588236 -0.5\n",
      " -0.48823529 -0.5        -0.5        -0.5        -0.49215686 -0.5\n",
      " -0.28039217  0.5        -0.01372549 -0.5        -0.48039216 -0.5\n",
      " -0.21372549  0.5         0.49607843  0.49607843  0.5         0.5         0.5\n",
      "  0.5         0.5         0.5         0.5         0.5         0.48431373\n",
      "  0.5        -0.26078433 -0.5        -0.48823529 -0.5        -0.5        -0.5\n",
      " -0.49215686 -0.5        -0.38235295  0.5         0.10392157 -0.5\n",
      " -0.48431373 -0.5        -0.26078433  0.46078432  0.49215686  0.48039216\n",
      "  0.5         0.44117647  0.30392158  0.29215688  0.29215688  0.30000001\n",
      "  0.31176472  0.31960785  0.33137256  0.33137256 -0.3392157  -0.5\n",
      " -0.49215686 -0.5        -0.5        -0.5        -0.49607843 -0.5\n",
      " -0.42156863  0.44117647  0.21764706 -0.5        -0.49215686 -0.5\n",
      " -0.28039217  0.45294118  0.5         0.48039216  0.5         0.2254902\n",
      " -0.5        -0.49607843 -0.49607843 -0.5        -0.5        -0.49215686\n",
      " -0.48823529 -0.48431373 -0.49607843 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.49607843 -0.5        -0.44509804  0.35490197  0.31568629\n",
      " -0.48823529 -0.49607843 -0.5        -0.30000001  0.43333334  0.5\n",
      "  0.48039216  0.5         0.22156863 -0.5        -0.49607843 -0.49607843\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.49607843 -0.5\n",
      " -0.46862745  0.28039217  0.40196079 -0.47647059 -0.5        -0.5\n",
      " -0.32352942  0.4137255   0.5         0.48039216  0.5         0.23333333\n",
      " -0.5        -0.49607843 -0.49607843 -0.5        -0.49607843 -0.49607843\n",
      " -0.49607843 -0.49607843 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.49607843 -0.5        -0.48431373  0.21372549  0.46862745\n",
      " -0.45294118 -0.48823529 -0.5        -0.33137256  0.39803922  0.5\n",
      "  0.48039216  0.5         0.25294119 -0.49215686 -0.5        -0.49607843\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.49607843 -0.5\n",
      " -0.49215686  0.17843138  0.49215686 -0.46470588 -0.5        -0.5\n",
      " -0.37450981  0.37058824  0.5         0.48039216  0.5         0.2764706\n",
      " -0.46470588 -0.5        -0.49607843 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.49607843 -0.5        -0.49215686  0.18235295  0.5         0.08039216\n",
      "  0.06862745  0.09607843  0.17450981  0.44509804  0.49607843  0.47254902\n",
      "  0.5         0.29215688 -0.44117647 -0.5        -0.49607843 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.30000001\n",
      "  0.13137256  0.49607843  0.5         0.5         0.5         0.5         0.5\n",
      "  0.48823529  0.5         0.33137256 -0.40980393 -0.5        -0.49215686\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.49607843 -0.5        -0.37450981  0.05686275  0.24117647  0.20196079\n",
      "  0.17843138  0.14705883  0.11176471  0.1        -0.04117647 -0.45294118\n",
      " -0.5        -0.49607843 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(train_labels[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PROBLEMA #1\n",
    "# Implementare una logistic regression multinomiale con discesa del gradiente \n",
    "# con Tensorflow (TF) come classificatore per notMNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJKnDcJAkdRgOkqSO/w86rb/xrEX6zwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1137379e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# La matrice dei pensi W viene spesso inizializzata con una variabile casuale con distribuzione normale,\n",
    "# dove i valori maggiori di 2 x std_dev sono rimossi.\n",
    "# weights = tf.Variable( tf.truncated_normal(...))\n",
    "\n",
    "# Questo permette di ignorare valori troppo grandi o piccoli che possono influenzare negativamente l'apprendimento.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "n = 500000\n",
    "A = tf.truncated_normal((n,))\n",
    "B = tf.random_normal((n,))\n",
    "with tf.Session() as sess:\n",
    "    a, b = sess.run([A, B])\n",
    "\n",
    "plt.hist(a, 100, (-4.2, 4.2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e6aeda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(b, 100, (-4.2, 4.2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prendiamo un subset per limitare il tempo per l'addetramento, diminuiscilo se occorre\n",
    "train_subset = 10000\n",
    "\n",
    "# Su Tensorflow ogni elemento - input, variabili ed elaborazioni - è descritto mediante un grafo, o dataflow graph.\n",
    "# Gli oggetti tf.Operation rappresentano unità di computazione;\n",
    "# Gli oggetti tf.Tensor rappresentano unità di dati (tensori) che sono usati come input e output per gli oggetti Operation.\n",
    "\n",
    "# In TF un grafo tf.Graph contiene due tipi di informazione:\n",
    "# La struttura: nodi e archi che rappresentano le operazioni \n",
    "# Le collections: insiemi di metadati (inseriti con tf.add_to_collection) nella forma <chiave,lista di objects); si può ispezionare con tf.get_collection.\n",
    "\n",
    "# TF usa questa struttura per salvare variabili e altre informazioni del grafo.\n",
    "\n",
    "# Un oggetto Graph di default è sempre prensente e accedibile chiamando tf.get_default_graph. \n",
    "\n",
    "# Un approccio alternativo per usare i grafo di Tensorflow consiste nel context manager tf.Graph.as_default, \n",
    "# che sostituisce il grafo di default per tutta l'esistenza del contesto in esame.\n",
    "graph = tf.Graph()\n",
    "\n",
    "# Costruisco un grafo di computazione con Tensorflow\n",
    "with graph.as_default():\n",
    "\n",
    "  # Creo tensori costanti per i seguenti set: trainig, test e validation\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  # per assegnare un nome alla variabile possiamo usare il secondo parametro, e.g., tf.constant(0, name=\"c\") \n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Le variabili mantengono lo stato durante le elaborazioni. Sono anch'esse tensori.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  # Il vettore di bias b è inizializzato a 0.\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Calcolo Wx + b\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "\n",
    "  # La funzione softmax_cross_entropy_with_logits valuta la funzione di loss\n",
    "  # per mezzo della cross-entropy loss con l'output corretto (tf_train_labels)\n",
    "  # Mentre reduce_mean valuta semplicemente la media dei valori del tensore.\n",
    "  # loss indica una operazione TF.\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Instanzio un algoritmo di discesa del gradiente con learning rate = 0.5 (alfa nelle slide di richiami sulle reti neurali.)\n",
    "  # La funzione minimize è composta di 2 elaborazioni: compute_gradients e apply_gradients.\n",
    "  # La prima ricava i gradienti, la seconda aggiorna la matrice dei pesi di conseguenza.\n",
    "  # optimizer indica una operazione TF.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "  # calcolo softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), dim) per i sets: \n",
    "  # training, validation e test.\n",
    "  # N.B.: i set valid e test sono usati solo per la valutazione, non c'è backprop\n",
    "  # N.B.(2): ci servono per valutare l'accuratezza, l'apprendimento l'abbiamo già fatto.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  logits = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "  valid_prediction = tf.nn.softmax(logits) \n",
    "  logits = tf.matmul(tf.matmul(tf_test_dataset, weights) + biases\n",
    "  test_prediction = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 16.911768\n",
      "Training accuracy: 8.6%\n",
      "Validation accuracy: 11.8%\n",
      "Loss at step 100: 2.283592\n",
      "Training accuracy: 72.0%\n",
      "Validation accuracy: 72.3%\n",
      "Loss at step 200: 1.839189\n",
      "Training accuracy: 74.8%\n",
      "Validation accuracy: 74.7%\n",
      "Loss at step 300: 1.599836\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 75.3%\n",
      "Loss at step 400: 1.439709\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 75.6%\n",
      "Loss at step 500: 1.321766\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 76.0%\n",
      "Loss at step 600: 1.229986\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 76.1%\n",
      "Loss at step 700: 1.155922\n",
      "Training accuracy: 78.8%\n",
      "Validation accuracy: 76.2%\n",
      "Loss at step 800: 1.094414\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 76.3%\n",
      "Test accuracy: 82.9%\n"
     ]
    }
   ],
   "source": [
    "# numero cicli di elaborazione\n",
    "num_steps = 801\n",
    "\n",
    "# Definisco l'accuratezza come somma del numero di predizioni corrette \n",
    "# normalizzato sul numero di predizioni totali.\n",
    "# La uso per fare statistiche durante il funzionamento.\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "# TF usa tf.Session per rappresentare una connesione tra il programa e il runtime C++.\n",
    "# Serve per creare un ambiente in cui lanciare le operazioni definite nel grafo.\n",
    "# Poichè la classe alloca risorse fisiche, solitamente si usa come context manager (dentro un blocco with),\n",
    "# che le libera automaticamente al termine del blocco, cioè lancia session.close() al termine della esecuzione.\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # Istanzia e lancia una operazione per l'inizializzazione delle variabili globali del grafo\n",
    "  # cioè: weights e biases. Va eseguita solo una volta.\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "      \n",
    "  for step in range(num_steps):\n",
    "    # Eseguo le operazioni nel grafo.\n",
    "    # Le operazioni e i tensori da valutare sono definiti nel primo parametro, un NumPy array.\n",
    "    # La lista indica le foglie grafo.\n",
    "    # Il valore di ritorno ha lo stesso tipo dell'input, cioè un array, \n",
    "    # dove le foglie sono sostituite con il corrispondente valore calcolato da TF.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    # ogni tanto stampo statistiche\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Se invoco eval() su valid_prediction, sto calcolando l'operazione sui \n",
    "      # pesi e bias correnti.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  # Al termine stamo l'accuracy sul test set.\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PROBLEMA #2\n",
    "# Prova a modificare il codice precedente impiegando un Stochastic gradient descent.\n",
    "# Quanto tempo impiega ora per terminare l'elaborazione?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  6.]\n"
     ]
    }
   ],
   "source": [
    "# Durante l'elaborazone batch l'algoritmo elabora solo un sottoinsieme di dati alla volta.\n",
    "# L'elaborazione è ripetuta, perciò conviene scrivere il codice \n",
    "# senza gestire la creazione dei dati direttamente.\n",
    "# In TF un placeholder è una variabile che assumera i valori a tempo di esecuzione.\n",
    "# Possiamo costruire il grafo delle operazioni senza il bisogno di conoscere i dati.\n",
    "\n",
    "# Nel seguente codice creiamo una operazione (y) di moltiplicazione * 2 senza sapere i valori.\n",
    "# Ora la possiamo eseguire all'interno di una sessione. Per valutarla occorre fornire (feed)\n",
    "# i valori per x. \n",
    "# None significa che non poniamo vincoli sulla dimensione.\n",
    "x = tf.placeholder(tf.float32, shape=None)\n",
    "y = x * 2\n",
    "\n",
    "# TF supporta tipi di variabili simili a NumPy (es. float32, float64, int32, int64)\n",
    "# https://docs.scipy.org/doc/numpy-1.13.0/user/basics.types.html\n",
    "\n",
    "with tf.Session() as session:\n",
    "    result = session.run(y, feed_dict={x: [1, 2, 3]})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N.B. Fuori dallo scope session non possiamo stampare il valore dei tensori \n",
    "# durante l'elaborazione.\n",
    "\n",
    "# x = tf.placeholder(\"float\", None)\n",
    "# y = x * 2\n",
    "# print(x) \n",
    "\n",
    "# Output: \"Tensor(\"Placeholder_11:0\", dtype=float32)\" \n",
    "# Stampa solo il tipo e non il valore di x.\n",
    "# In alternativa usare https://www.tensorflow.org/api_docs/python/tf/InteractiveSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.   4.   6.]\n",
      " [  8.  10.  12.]]\n"
     ]
    }
   ],
   "source": [
    "# Possiamo dare in input anche strutture più complesse indicando il formato dei dati con shape.\n",
    "# Es. un qualsiasi numero di righe, ma il numero di colonne pari a 3\n",
    "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = x * 2\n",
    "\n",
    "with tf.Session() as session:\n",
    "    x_data = [[1, 2, 3],\n",
    "              [4, 5, 6],]\n",
    "    result = session.run(y, feed_dict={x: x_data})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Una immagine a colori (RGB) in formato raw può avere una rappresentazione matriciale\n",
    "#image = tf.placeholder(\"uint8\", shape=[None, None, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Una operazione placeholder viene usata per alimentare il grafo.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "\n",
    "  # Il resto è uguale al precedente esempio.\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  logits = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "  valid_prediction = tf.nn.softmax(logits) \n",
    "  logits = tf.matmul(tf.matmul(tf_test_dataset, weights) + biases\n",
    "  test_prediction = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 14.712099\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 15.6%\n",
      "Minibatch loss at step 500: 1.678132\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 1000: 1.159136\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 1500: 1.157310\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 2000: 0.961426\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 2500: 1.139208\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 3000: 1.100741\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 79.2%\n",
      "Test accuracy: 85.8%\n"
     ]
    }
   ],
   "source": [
    "# Se impiego minibatch potenzialmente ho più varianza nell'apprendimento ad ogni ciclo.\n",
    "# Sono costretto ad aumentare gli step.\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "    \n",
    "  for step in range(num_steps):\n",
    "    # Definisco un offset nel trainig set\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    \n",
    "    # Estraggo ilminibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "    # Dizionario {chiave_placeholder : valore, ...}\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    \n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PROBLEMA #3\n",
    "# Usando l'help online di TF prova a creare una rete neurale con 1-hidden layer\n",
    "# con attivazione RELU e 1024 nodi nascosti.\n",
    "\n",
    "# N.B. la funzione tf.nn.relu() restituisce un tensore che calcola la RELU sul tensore di input.\n",
    "# L'output ha la stessa dimensione dell'input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nodi del hidden layer\n",
    "hidden_nodes= 1024\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Avendo un layer in più ho due coppie <W,B>\n",
    "    # N.B. W1 e B1 hanno dimensioni <#input-feature-vector,#hidden-nodes>, <#hidden-nodes>\n",
    "    # mentre W2 <#hidden-nodes,num_labels>, <num_labels>\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "    biases_1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    \n",
    "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "\n",
    "    # Ora la loss function è definita sullo layer di output\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits \\\n",
    "                          (labels=tf_train_labels, logits=logits_2))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits_2)\n",
    "    # Seguo la stessa pipeline per valid e test set\n",
    "    logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    valid_prediction = tf.nn.softmax(logits_2) \n",
    "    \n",
    "    logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    test_prediction = tf.nn.softmax(logits_2)                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 380.785950\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 27.6%\n",
      "Minibatch loss at step 500: 14.854194\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 1000: 13.042046\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 1500: 8.946371\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 2000: 3.762165\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 2500: 3.894309\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 3000: 2.647440\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.5%\n",
      "Test accuracy: 86.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "    \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    \n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    \n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
