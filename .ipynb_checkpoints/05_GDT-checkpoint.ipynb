{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unita: GDT\n",
    "\n",
    "## Semplice classificazione basata su Logistic Regression e Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in seguito ci servono\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# partiamo dall'output dell'unit√† 04\n",
    "# folder dove e' posizionato il file pickle\n",
    "pickle_file = \"../datasets/notMNIST.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# carico i dati\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  saved = pickle.load(f)\n",
    "  train_dataset = saved['train_dataset']\n",
    "  train_labels = saved['train_labels']\n",
    "  valid_dataset = saved['valid_dataset']\n",
    "  valid_labels = saved['valid_labels']\n",
    "  test_dataset = saved['test_dataset']\n",
    "  test_labels = saved['test_labels']\n",
    "  del saved  # garbage collector per liberare memoria\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14313726  0.37450981  0.22941177  0.20196079  0.18235295  0.15490197\n",
      "   0.13137256  0.10392157  0.07647059  0.0372549  -0.00196078 -0.03333334\n",
      "  -0.06470589 -0.08431373 -0.10392157 -0.10392157 -0.08039216 -0.05686275\n",
      "  -0.03333334  0.00588235  0.08039216  0.13921569  0.18235295  0.23333333\n",
      "  -0.10784314 -0.48039216 -0.5        -0.5       ]\n",
      " [-0.19803922  0.5        -0.04117647 -0.34705883 -0.27254903 -0.24509804\n",
      "  -0.21372549 -0.18235295 -0.15490197 -0.11568628 -0.07647059 -0.04509804\n",
      "  -0.02156863 -0.00588235  0.00196078  0.00980392 -0.00196078 -0.00980392\n",
      "  -0.0254902  -0.06470589 -0.12745099 -0.19019608  0.35490197  0.5         0.5\n",
      "   0.3392157  -0.04509804 -0.43333334]\n",
      " [-0.28431374  0.5        -0.0254902  -0.5        -0.49215686 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.49607843\n",
      "  -0.5        -0.44509804  0.32745099  0.5         0.48823529  0.48431373\n",
      "   0.5        -0.20980392]\n",
      " [-0.3509804   0.43725491  0.14705883 -0.5        -0.48823529 -0.49215686\n",
      "  -0.48823529 -0.48823529 -0.48823529 -0.48823529 -0.48431373 -0.48431373\n",
      "  -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373\n",
      "  -0.48431373 -0.47254902 -0.5        -0.28823531  0.42941177  0.5\n",
      "   0.48823529  0.5         0.42941177 -0.31568629]\n",
      " [-0.41764706  0.34705883  0.29607844 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.48823529 -0.48431373 -0.48431373 -0.48431373 -0.48431373\n",
      "  -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373\n",
      "  -0.47254902 -0.5        -0.18627451  0.5         0.5         0.48823529\n",
      "   0.5         0.33529413 -0.39411765]\n",
      " [-0.48431373  0.25686276  0.42156863 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.48039216\n",
      "  -0.5        -0.07647059  0.5         0.48431373  0.48431373  0.5\n",
      "   0.2254902  -0.47254902]\n",
      " [-0.5         0.15490197  0.48823529 -0.4254902  -0.5        -0.49607843\n",
      "  -0.5        -0.48039216 -0.16666667 -0.08039216 -0.07647059 -0.05686275\n",
      "  -0.04117647 -0.03333334 -0.0254902  -0.0372549  -0.06078431 -0.08039216\n",
      "  -0.11176471 -0.17058824 -0.30000001  0.11960784  0.5         0.47647059\n",
      "   0.47647059  0.5         0.11960784 -0.5       ]\n",
      " [-0.5         0.06078431  0.5        -0.31568629 -0.5        -0.48823529\n",
      "  -0.5        -0.44901961  0.31568629  0.5         0.49215686  0.5         0.5\n",
      "   0.5         0.5         0.5         0.5         0.5         0.5         0.5\n",
      "   0.5         0.5         0.5         0.5         0.48431373  0.5\n",
      "   0.00980392 -0.5       ]\n",
      " [-0.5        -0.05294118  0.5        -0.19411765 -0.5        -0.48431373\n",
      "  -0.5        -0.47647059  0.21372549  0.5         0.47254902  0.48823529\n",
      "   0.47647059  0.31568629  0.25294119  0.2372549   0.21372549  0.19411765\n",
      "   0.19803922  0.2254902   0.25686276  0.28823531  0.3392157   0.39803922\n",
      "   0.44117647  0.5        -0.11960784 -0.5       ]\n",
      " [-0.5        -0.17058824  0.5        -0.07254902 -0.5        -0.48039216\n",
      "  -0.5        -0.49607843  0.16666667  0.5         0.48431373  0.5\n",
      "   0.46862745 -0.27254903 -0.5        -0.49215686 -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.49607843 -0.46078432 -0.39411765\n",
      "  -0.33529413 -0.44117647 -0.5       ]\n",
      " [-0.5        -0.28431374  0.5         0.04901961 -0.5        -0.47647059\n",
      "  -0.5        -0.5         0.10784314  0.5         0.48431373  0.5\n",
      "   0.48431373 -0.21372549 -0.5        -0.48431373 -0.49215686 -0.49215686\n",
      "  -0.48823529 -0.49215686 -0.49607843 -0.49607843 -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.39019608  0.49607843  0.15490197 -0.5        -0.48431373\n",
      "  -0.49215686 -0.5         0.05686275  0.5         0.47647059  0.5\n",
      "   0.49607843 -0.1627451  -0.5        -0.47254902 -0.48823529 -0.5        -0.5\n",
      "  -0.49215686 -0.48431373 -0.49607843 -0.49607843 -0.49607843 -0.49215686\n",
      "  -0.49215686 -0.49607843 -0.5       ]\n",
      " [-0.5        -0.49607843  0.43333334  0.25294119 -0.5        -0.5\n",
      "  -0.48823529 -0.5         0.00196078  0.5         0.48039216  0.5\n",
      "   0.49607843 -0.1627451  -0.5        -0.48823529 -0.5        -0.5\n",
      "  -0.45294118 -0.46470588 -0.5        -0.49607843 -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5         0.34705883  0.34313726 -0.43333334 -0.5\n",
      "  -0.48039216 -0.5        -0.06470589  0.5         0.47254902  0.5         0.5\n",
      "   0.23333333  0.05294118  0.13137256  0.16666667  0.25294119  0.41764706\n",
      "   0.2647059  -0.18627451 -0.46862745 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5         0.20980392  0.4254902  -0.36666667 -0.5\n",
      "  -0.48823529 -0.5        -0.44117647 -0.33137256 -0.32352942 -0.29607844\n",
      "  -0.2764706  -0.21372549 -0.18627451 -0.20196079 -0.31176472  0.10392157\n",
      "   0.5         0.49215686  0.5         0.36274511 -0.20980392 -0.5\n",
      "  -0.49607843 -0.5        -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5         0.07647059  0.5        -0.30000001 -0.5\n",
      "  -0.48823529 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.48823529 -0.5         0.09607843  0.5\n",
      "   0.48039216  0.47647059  0.5        -0.00588235 -0.5        -0.49607843\n",
      "  -0.5        -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5        -0.05294118  0.5        -0.23333333 -0.5\n",
      "  -0.48431373 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.48823529 -0.5         0.21764706  0.5\n",
      "   0.49215686  0.48823529  0.5        -0.12745099 -0.5        -0.49215686\n",
      "  -0.5        -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5        -0.16666667  0.5        -0.13529412 -0.5\n",
      "  -0.48431373 -0.5        -0.40196079 -0.1509804  -0.14705883 -0.13921569\n",
      "  -0.13137256 -0.13529412 -0.14313726 -0.14313726 -0.20196079  0.3509804\n",
      "   0.5         0.48431373  0.47647059  0.5        -0.20588236 -0.5\n",
      "  -0.48823529 -0.5        -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5        -0.28039217  0.5        -0.01372549 -0.5\n",
      "  -0.48039216 -0.5        -0.21372549  0.5         0.49607843  0.49607843\n",
      "   0.5         0.5         0.5         0.5         0.5         0.5         0.5\n",
      "   0.5         0.48431373  0.5        -0.26078433 -0.5        -0.48823529\n",
      "  -0.5        -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5        -0.38235295  0.5         0.10392157 -0.5\n",
      "  -0.48431373 -0.5        -0.26078433  0.46078432  0.49215686  0.48039216\n",
      "   0.5         0.44117647  0.30392158  0.29215688  0.29215688  0.30000001\n",
      "   0.31176472  0.31960785  0.33137256  0.33137256 -0.3392157  -0.5\n",
      "  -0.49215686 -0.5        -0.5        -0.5       ]\n",
      " [-0.49607843 -0.5        -0.42156863  0.44117647  0.21764706 -0.5\n",
      "  -0.49215686 -0.5        -0.28039217  0.45294118  0.5         0.48039216\n",
      "   0.5         0.2254902  -0.5        -0.49607843 -0.49607843 -0.5        -0.5\n",
      "  -0.49215686 -0.48823529 -0.48431373 -0.49607843 -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.49607843 -0.5        -0.44509804  0.35490197  0.31568629 -0.48823529\n",
      "  -0.49607843 -0.5        -0.30000001  0.43333334  0.5         0.48039216\n",
      "   0.5         0.22156863 -0.5        -0.49607843 -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.49607843 -0.5        -0.46862745  0.28039217  0.40196079 -0.47647059\n",
      "  -0.5        -0.5        -0.32352942  0.4137255   0.5         0.48039216\n",
      "   0.5         0.23333333 -0.5        -0.49607843 -0.49607843 -0.5\n",
      "  -0.49607843 -0.49607843 -0.49607843 -0.49607843 -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5       ]\n",
      " [-0.49607843 -0.5        -0.48431373  0.21372549  0.46862745 -0.45294118\n",
      "  -0.48823529 -0.5        -0.33137256  0.39803922  0.5         0.48039216\n",
      "   0.5         0.25294119 -0.49215686 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.49607843 -0.5        -0.49215686  0.17843138  0.49215686 -0.46470588\n",
      "  -0.5        -0.5        -0.37450981  0.37058824  0.5         0.48039216\n",
      "   0.5         0.2764706  -0.46470588 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.49607843 -0.5        -0.49215686  0.18235295  0.5         0.08039216\n",
      "   0.06862745  0.09607843  0.17450981  0.44509804  0.49607843  0.47254902\n",
      "   0.5         0.29215688 -0.44117647 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.30000001  0.13137256  0.49607843\n",
      "   0.5         0.5         0.5         0.5         0.5         0.48823529\n",
      "   0.5         0.33137256 -0.40980393 -0.5        -0.49215686 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.49607843 -0.5        -0.37450981\n",
      "   0.05686275  0.24117647  0.20196079  0.17843138  0.14705883  0.11176471\n",
      "   0.1        -0.04117647 -0.45294118 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]]\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(train_labels[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Riportiamo i dati nel formato adatto al processamento: matrix 1-dim + vettore 1-hot encoding\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  # -1 indica che la dimensione iniziale rimane invariata\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  # aggiungo una dimensione a labels\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.14313726  0.37450981  0.22941177  0.20196079  0.18235295  0.15490197\n",
      "  0.13137256  0.10392157  0.07647059  0.0372549  -0.00196078 -0.03333334\n",
      " -0.06470589 -0.08431373 -0.10392157 -0.10392157 -0.08039216 -0.05686275\n",
      " -0.03333334  0.00588235  0.08039216  0.13921569  0.18235295  0.23333333\n",
      " -0.10784314 -0.48039216 -0.5        -0.5        -0.19803922  0.5\n",
      " -0.04117647 -0.34705883 -0.27254903 -0.24509804 -0.21372549 -0.18235295\n",
      " -0.15490197 -0.11568628 -0.07647059 -0.04509804 -0.02156863 -0.00588235\n",
      "  0.00196078  0.00980392 -0.00196078 -0.00980392 -0.0254902  -0.06470589\n",
      " -0.12745099 -0.19019608  0.35490197  0.5         0.5         0.3392157\n",
      " -0.04509804 -0.43333334 -0.28431374  0.5        -0.0254902  -0.5\n",
      " -0.49215686 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.49607843 -0.5        -0.44509804  0.32745099  0.5\n",
      "  0.48823529  0.48431373  0.5        -0.20980392 -0.3509804   0.43725491\n",
      "  0.14705883 -0.5        -0.48823529 -0.49215686 -0.48823529 -0.48823529\n",
      " -0.48823529 -0.48823529 -0.48431373 -0.48431373 -0.48431373 -0.48431373\n",
      " -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.47254902\n",
      " -0.5        -0.28823531  0.42941177  0.5         0.48823529  0.5\n",
      "  0.42941177 -0.31568629 -0.41764706  0.34705883  0.29607844 -0.5\n",
      " -0.49607843 -0.5        -0.5        -0.5        -0.48823529 -0.48431373\n",
      " -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373\n",
      " -0.48431373 -0.48431373 -0.48431373 -0.47254902 -0.5        -0.18627451\n",
      "  0.5         0.5         0.48823529  0.5         0.33529413 -0.39411765\n",
      " -0.48431373  0.25686276  0.42156863 -0.5        -0.49607843 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.48039216\n",
      " -0.5        -0.07647059  0.5         0.48431373  0.48431373  0.5\n",
      "  0.2254902  -0.47254902 -0.5         0.15490197  0.48823529 -0.4254902\n",
      " -0.5        -0.49607843 -0.5        -0.48039216 -0.16666667 -0.08039216\n",
      " -0.07647059 -0.05686275 -0.04117647 -0.03333334 -0.0254902  -0.0372549\n",
      " -0.06078431 -0.08039216 -0.11176471 -0.17058824 -0.30000001  0.11960784\n",
      "  0.5         0.47647059  0.47647059  0.5         0.11960784 -0.5        -0.5\n",
      "  0.06078431  0.5        -0.31568629 -0.5        -0.48823529 -0.5\n",
      " -0.44901961  0.31568629  0.5         0.49215686  0.5         0.5         0.5\n",
      "  0.5         0.5         0.5         0.5         0.5         0.5         0.5\n",
      "  0.5         0.5         0.5         0.48431373  0.5         0.00980392\n",
      " -0.5        -0.5        -0.05294118  0.5        -0.19411765 -0.5\n",
      " -0.48431373 -0.5        -0.47647059  0.21372549  0.5         0.47254902\n",
      "  0.48823529  0.47647059  0.31568629  0.25294119  0.2372549   0.21372549\n",
      "  0.19411765  0.19803922  0.2254902   0.25686276  0.28823531  0.3392157\n",
      "  0.39803922  0.44117647  0.5        -0.11960784 -0.5        -0.5\n",
      " -0.17058824  0.5        -0.07254902 -0.5        -0.48039216 -0.5\n",
      " -0.49607843  0.16666667  0.5         0.48431373  0.5         0.46862745\n",
      " -0.27254903 -0.5        -0.49215686 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.49607843 -0.46078432 -0.39411765 -0.33529413\n",
      " -0.44117647 -0.5        -0.5        -0.28431374  0.5         0.04901961\n",
      " -0.5        -0.47647059 -0.5        -0.5         0.10784314  0.5\n",
      "  0.48431373  0.5         0.48431373 -0.21372549 -0.5        -0.48431373\n",
      " -0.49215686 -0.49215686 -0.48823529 -0.49215686 -0.49607843 -0.49607843\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.39019608  0.49607843  0.15490197 -0.5        -0.48431373 -0.49215686\n",
      " -0.5         0.05686275  0.5         0.47647059  0.5         0.49607843\n",
      " -0.1627451  -0.5        -0.47254902 -0.48823529 -0.5        -0.5\n",
      " -0.49215686 -0.48431373 -0.49607843 -0.49607843 -0.49607843 -0.49215686\n",
      " -0.49215686 -0.49607843 -0.5        -0.5        -0.49607843  0.43333334\n",
      "  0.25294119 -0.5        -0.5        -0.48823529 -0.5         0.00196078\n",
      "  0.5         0.48039216  0.5         0.49607843 -0.1627451  -0.5\n",
      " -0.48823529 -0.5        -0.5        -0.45294118 -0.46470588 -0.5\n",
      " -0.49607843 -0.49607843 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.49215686 -0.5         0.34705883  0.34313726 -0.43333334 -0.5\n",
      " -0.48039216 -0.5        -0.06470589  0.5         0.47254902  0.5         0.5\n",
      "  0.23333333  0.05294118  0.13137256  0.16666667  0.25294119  0.41764706\n",
      "  0.2647059  -0.18627451 -0.46862745 -0.5        -0.49607843 -0.5        -0.5\n",
      " -0.5        -0.5        -0.49215686 -0.5         0.20980392  0.4254902\n",
      " -0.36666667 -0.5        -0.48823529 -0.5        -0.44117647 -0.33137256\n",
      " -0.32352942 -0.29607844 -0.2764706  -0.21372549 -0.18627451 -0.20196079\n",
      " -0.31176472  0.10392157  0.5         0.49215686  0.5         0.36274511\n",
      " -0.20980392 -0.5        -0.49607843 -0.5        -0.5        -0.5\n",
      " -0.49215686 -0.5         0.07647059  0.5        -0.30000001 -0.5\n",
      " -0.48823529 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.48823529 -0.5         0.09607843  0.5\n",
      "  0.48039216  0.47647059  0.5        -0.00588235 -0.5        -0.49607843\n",
      " -0.5        -0.5        -0.5        -0.49215686 -0.5        -0.05294118\n",
      "  0.5        -0.23333333 -0.5        -0.48431373 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.48823529\n",
      " -0.5         0.21764706  0.5         0.49215686  0.48823529  0.5\n",
      " -0.12745099 -0.5        -0.49215686 -0.5        -0.5        -0.5\n",
      " -0.49215686 -0.5        -0.16666667  0.5        -0.13529412 -0.5\n",
      " -0.48431373 -0.5        -0.40196079 -0.1509804  -0.14705883 -0.13921569\n",
      " -0.13137256 -0.13529412 -0.14313726 -0.14313726 -0.20196079  0.3509804\n",
      "  0.5         0.48431373  0.47647059  0.5        -0.20588236 -0.5\n",
      " -0.48823529 -0.5        -0.5        -0.5        -0.49215686 -0.5\n",
      " -0.28039217  0.5        -0.01372549 -0.5        -0.48039216 -0.5\n",
      " -0.21372549  0.5         0.49607843  0.49607843  0.5         0.5         0.5\n",
      "  0.5         0.5         0.5         0.5         0.5         0.48431373\n",
      "  0.5        -0.26078433 -0.5        -0.48823529 -0.5        -0.5        -0.5\n",
      " -0.49215686 -0.5        -0.38235295  0.5         0.10392157 -0.5\n",
      " -0.48431373 -0.5        -0.26078433  0.46078432  0.49215686  0.48039216\n",
      "  0.5         0.44117647  0.30392158  0.29215688  0.29215688  0.30000001\n",
      "  0.31176472  0.31960785  0.33137256  0.33137256 -0.3392157  -0.5\n",
      " -0.49215686 -0.5        -0.5        -0.5        -0.49607843 -0.5\n",
      " -0.42156863  0.44117647  0.21764706 -0.5        -0.49215686 -0.5\n",
      " -0.28039217  0.45294118  0.5         0.48039216  0.5         0.2254902\n",
      " -0.5        -0.49607843 -0.49607843 -0.5        -0.5        -0.49215686\n",
      " -0.48823529 -0.48431373 -0.49607843 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.49607843 -0.5        -0.44509804  0.35490197  0.31568629\n",
      " -0.48823529 -0.49607843 -0.5        -0.30000001  0.43333334  0.5\n",
      "  0.48039216  0.5         0.22156863 -0.5        -0.49607843 -0.49607843\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.49607843 -0.5\n",
      " -0.46862745  0.28039217  0.40196079 -0.47647059 -0.5        -0.5\n",
      " -0.32352942  0.4137255   0.5         0.48039216  0.5         0.23333333\n",
      " -0.5        -0.49607843 -0.49607843 -0.5        -0.49607843 -0.49607843\n",
      " -0.49607843 -0.49607843 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.49607843 -0.5        -0.48431373  0.21372549  0.46862745\n",
      " -0.45294118 -0.48823529 -0.5        -0.33137256  0.39803922  0.5\n",
      "  0.48039216  0.5         0.25294119 -0.49215686 -0.5        -0.49607843\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.49607843 -0.5\n",
      " -0.49215686  0.17843138  0.49215686 -0.46470588 -0.5        -0.5\n",
      " -0.37450981  0.37058824  0.5         0.48039216  0.5         0.2764706\n",
      " -0.46470588 -0.5        -0.49607843 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.49607843 -0.5        -0.49215686  0.18235295  0.5         0.08039216\n",
      "  0.06862745  0.09607843  0.17450981  0.44509804  0.49607843  0.47254902\n",
      "  0.5         0.29215688 -0.44117647 -0.5        -0.49607843 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.30000001\n",
      "  0.13137256  0.49607843  0.5         0.5         0.5         0.5         0.5\n",
      "  0.48823529  0.5         0.33137256 -0.40980393 -0.5        -0.49215686\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.49607843 -0.5        -0.37450981  0.05686275  0.24117647  0.20196079\n",
      "  0.17843138  0.14705883  0.11176471  0.1        -0.04117647 -0.45294118\n",
      " -0.5        -0.49607843 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(train_labels[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PROBLEMA #1\n",
    "# Implementare una logistic regression multinomiale con discesa del gradiente \n",
    "# con Tensorflow (TF) come classificatore per notMNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prendiamo un subset per limitare il tempo per l'addetramento, diminuiscilo se occorre\n",
    "train_subset = 10000\n",
    "\n",
    "# Su Tensorflow ogni elemento - input, variabili ed elaborazioni - √® descritto mediante un grafo, o dataflow graph.\n",
    "# Gli oggetti tf.Operation rappresentano unit√† di computazione;\n",
    "# Gli oggetti tf.Tensor rappresentano unit√† di dati (tensori) che sono usati come input e output per gli oggetti Operation.\n",
    "\n",
    "# In TF un grafo tf.Graph contiene due tipi di informazione:\n",
    "# La struttura: nodi e archi che rappresentano le operazioni \n",
    "# Le collections: insiemi di metadati (inseriti con tf.add_to_collection) nella forma <chiave,lista di objects); si pu√≤ ispezionare con tf.get_collection.\n",
    "\n",
    "# TF usa questa struttura per salvare variabili e altre informazioni del grafo.\n",
    "\n",
    "# Un oggetto Graph di default √® sempre prensente e accedibile chiamando tf.get_default_graph. \n",
    "\n",
    "# Un approccio alternativo per usare i grafo di Tensorflow consiste nel context manager tf.Graph.as_default, \n",
    "# che sostituisce il grafo di default per tutta l'esistenza del contesto in esame.\n",
    "graph = tf.Graph()\n",
    "\n",
    "# Costruisco un grafo di computazione con Tensorflow\n",
    "with graph.as_default():\n",
    "\n",
    "  # Creo tensori costanti per i seguenti set: trainig, test e validation\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  # per assegnare un nome alla variabile possiamo usare il secondo parametro, e.g., tf.constant(0, name=\"c\") \n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Le variabili mantengono lo stato durante le elaborazioni. Sono anch'esse tensori.\n",
    "  # La matrice dei pensi W viene inizializzata con una variabile casuale con distribuzione normale,\n",
    "  # dove i valori maggiori di 2 x std_dev sono rimossi.\n",
    "  # Il vettore di bias b √® inizializzato a 0.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Calcolo Wx + b\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "\n",
    "  # La funzione softmax_cross_entropy_with_logits valuta la funzione di loss\n",
    "  # per mezzo della cross-entropy loss con l'output corretto (tf_train_labels)\n",
    "  # Mentre reduce_mean valuta semplicemente la media dei valori del tensore.\n",
    "  # loss indica una operazione TF.\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Instanzio un algoritmo di discesa del gradiente con learning rage = 0.5 \n",
    "  # La funzione minimize √® composta di 2 elaborazioni: compute_gradients e apply_gradients.\n",
    "  # La prima ricava i gradienti, la seconda aggiorna la matrice dei pesi di conseguenza.\n",
    "  # optimizer indica una operazione TF.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "  # calcolo softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), dim) per i sets: \n",
    "  # training, validation e test.\n",
    "  # N.B.: i set valid e test sono usati solo per la valutazione, non c'√® backprop\n",
    "  # N.B.(2): ci servono per valutare l'accuratezza, l'apprendimento l'abbiamo gi√† fatto.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  logits = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "  valid_prediction = tf.nn.softmax(logits) \n",
    "  logits = tf.matmul(tf.matmul(tf_test_dataset, weights) + biases\n",
    "  test_prediction = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 16.911768\n",
      "Training accuracy: 8.6%\n",
      "Validation accuracy: 11.8%\n",
      "Loss at step 100: 2.283592\n",
      "Training accuracy: 72.0%\n",
      "Validation accuracy: 72.3%\n",
      "Loss at step 200: 1.839189\n",
      "Training accuracy: 74.8%\n",
      "Validation accuracy: 74.7%\n",
      "Loss at step 300: 1.599836\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 75.3%\n",
      "Loss at step 400: 1.439709\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 75.6%\n",
      "Loss at step 500: 1.321766\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 76.0%\n",
      "Loss at step 600: 1.229986\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 76.1%\n",
      "Loss at step 700: 1.155922\n",
      "Training accuracy: 78.8%\n",
      "Validation accuracy: 76.2%\n",
      "Loss at step 800: 1.094414\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 76.3%\n",
      "Test accuracy: 82.9%\n"
     ]
    }
   ],
   "source": [
    "# numero cicli di elaborazione\n",
    "num_steps = 801\n",
    "\n",
    "# Definisco l'accuratezza come somma del numero di predizioni corrette \n",
    "# normalizzato sul numero di predizioni totali.\n",
    "# La uso per fare statistiche durante il funzionamento.\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "# TF usa tf.Session per rappresentare una connesione tra il programa e il runtime C++.\n",
    "# Serve per creare un ambiente in cui lanciare le operazioni definite nel grafo.\n",
    "# Poich√® la classe alloca risorse fisiche, solitamente si usa come context manager (dentro un blocco with),\n",
    "# che le libera automaticamente al termine del blocco, cio√® lancia session.close() al termine della esecuzione.\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # Istanzia e lancia una operazione per l'inizializzazione delle variabili globali del grafo\n",
    "  # cio√®: weights e biases. Va eseguita solo una volta.\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "      \n",
    "  for step in range(num_steps):\n",
    "    # Eseguo le operazioni nel grafo.\n",
    "    # Le operazioni e i tensori da valutare sono definiti nel primo parametro, un NumPy array.\n",
    "    # La lista indica le foglie grafo.\n",
    "    # Il valore di ritorno ha lo stesso tipo dell'input, cio√® un array, \n",
    "    # dove le foglie sono sostituite con il corrispondente valore calcolato da TF.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    # ogni tanto stampo statistiche\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Se invoco eval() su valid_prediction, sto calcolando l'operazione sui \n",
    "      # pesi e bias correnti.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  # Al termine stamo l'accuracy sul test set.\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PROBLEMA #2\n",
    "# Prova a modificare il codice precedente impiegando un Stochastic gradient descent.\n",
    "# Quanto tempo impiega ora per terminare l'elaborazione?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  6.]\n"
     ]
    }
   ],
   "source": [
    "# Durante l'elaborazone batch l'algoritmo elabora solo un sottoinsieme di dati alla volta.\n",
    "# L'elaborazione √® ripetuta, perci√≤ conviene scrivere il codice \n",
    "# senza gestire la creazione dei dati direttamente.\n",
    "# In TF un placeholder √® una variabile che assumera i valori a tempo di esecuzione.\n",
    "# Possiamo costruire il grafo delle operazioni senza il bisogno di conoscere i dati.\n",
    "\n",
    "# Nel seguente codice creiamo una operazione (y) di moltiplicazione * 2 senza sapere i valori.\n",
    "# Ora la possiamo eseguire all'interno di una sessione. Per valutarla occorre fornire (feed)\n",
    "# i valori per x. \n",
    "# None significa che non poniamo vincoli sulla dimensione.\n",
    "x = tf.placeholder(tf.float32, shape=None)\n",
    "y = x * 2\n",
    "\n",
    "# TF supporta tipi di variabili simili a NumPy (es. float32, float64, int32, int64)\n",
    "# https://docs.scipy.org/doc/numpy-1.13.0/user/basics.types.html\n",
    "\n",
    "with tf.Session() as session:\n",
    "    result = session.run(y, feed_dict={x: [1, 2, 3]})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N.B. Fuori dallo scope session non possiamo stampare il valore dei tensori \n",
    "# durante l'elaborazione.\n",
    "\n",
    "# x = tf.placeholder(\"float\", None)\n",
    "# y = x * 2\n",
    "# print(x) \n",
    "\n",
    "# Output: \"Tensor(\"Placeholder_11:0\", dtype=float32)\" \n",
    "# Stampa solo il tipo e non il valore di x.\n",
    "# In alternativa usare https://www.tensorflow.org/api_docs/python/tf/InteractiveSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.   4.   6.]\n",
      " [  8.  10.  12.]]\n"
     ]
    }
   ],
   "source": [
    "# Possiamo dare in input anche strutture pi√π complesse indicando il formato dei dati con shape.\n",
    "# Es. un qualsiasi numero di righe, ma il numero di colonne pari a 3\n",
    "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = x * 2\n",
    "\n",
    "with tf.Session() as session:\n",
    "    x_data = [[1, 2, 3],\n",
    "              [4, 5, 6],]\n",
    "    result = session.run(y, feed_dict={x: x_data})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Una immagine a colori (RGB) in formato raw pu√≤ avere una rappresentazione matriciale\n",
    "#image = tf.placeholder(\"uint8\", shape=[None, None, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Una operazione placeholder viene usata per alimentare il grafo.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "\n",
    "  # Il resto √® uguale al precedente esempio.\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  logits = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "  valid_prediction = tf.nn.softmax(logits) \n",
    "  logits = tf.matmul(tf.matmul(tf_test_dataset, weights) + biases\n",
    "  test_prediction = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 14.712099\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 15.6%\n",
      "Minibatch loss at step 500: 1.678132\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 1000: 1.159136\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 1500: 1.157310\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 2000: 0.961426\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 2500: 1.139208\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 3000: 1.100741\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 79.2%\n",
      "Test accuracy: 85.8%\n"
     ]
    }
   ],
   "source": [
    "# Aumentiamo gli step\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "    \n",
    "  for step in range(num_steps):\n",
    "    # Definisco un offset nel trainig set\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    \n",
    "    # Estraggo ilminibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "    # Dizionario {chiave_placeholder : valore, ...}\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    \n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PROBLEMA #3\n",
    "# Usando l'help online di TF prova a creare una rete neurale con 1-hidden layer\n",
    "# con attivazione RELU e 1024 nodi nascosti.\n",
    "\n",
    "# Valuta la differenza dell'accuratezza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodi del hidden layer\n",
    "hidden_nodes= 1024\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Avendo un layer in pi√π ho due coppie <W,B>\n",
    "    # N.B. W1 e B1 hanno dimensioni <#input-feature-vector,#hidden-nodes>, <#hidden-nodes>\n",
    "    # mentre W2 <#hidden-nodes,num_labels>, <num_labels>\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "    biases_1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    \n",
    "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "\n",
    "    # Ora la loss function √® definita sullo layer di output\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits \\\n",
    "                          (labels=tf_train_labels, logits=logits_2))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits_2)\n",
    "    # Seguo la stessa pipeline per valid e test set\n",
    "    logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    valid_prediction = tf.nn.softmax(logits_2) \n",
    "    \n",
    "    logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    test_prediction = tf.nn.softmax(logits_2)                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 380.785950\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 27.6%\n",
      "Minibatch loss at step 500: 14.854194\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 1000: 13.042046\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 1500: 8.946371\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 2000: 3.762165\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 2500: 3.894309\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 3000: 2.647440\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.5%\n",
      "Test accuracy: 86.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "    \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    \n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    \n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
