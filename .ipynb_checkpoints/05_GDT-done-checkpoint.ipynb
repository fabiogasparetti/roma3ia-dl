{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unita: GDT\n",
    "\n",
    "## Semplice classificazione basata su Logistic Regression e Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in seguito ci servono\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# partiamo dall'output dell'unità 04\n",
    "# folder dove e' posizionato il file pickle\n",
    "pickle_file = \"../datasets/notMNIST.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# carico i dati\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  saved = pickle.load(f)\n",
    "  train_dataset = saved['train_dataset']\n",
    "  train_labels = saved['train_labels']\n",
    "  valid_dataset = saved['valid_dataset']\n",
    "  valid_labels = saved['valid_labels']\n",
    "  test_dataset = saved['test_dataset']\n",
    "  test_labels = saved['test_labels']\n",
    "  del saved  # garbage collector per liberare memoria\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14313726  0.37450981  0.22941177  0.20196079  0.18235295  0.15490197\n",
      "   0.13137256  0.10392157  0.07647059  0.0372549  -0.00196078 -0.03333334\n",
      "  -0.06470589 -0.08431373 -0.10392157 -0.10392157 -0.08039216 -0.05686275\n",
      "  -0.03333334  0.00588235  0.08039216  0.13921569  0.18235295  0.23333333\n",
      "  -0.10784314 -0.48039216 -0.5        -0.5       ]\n",
      " [-0.19803922  0.5        -0.04117647 -0.34705883 -0.27254903 -0.24509804\n",
      "  -0.21372549 -0.18235295 -0.15490197 -0.11568628 -0.07647059 -0.04509804\n",
      "  -0.02156863 -0.00588235  0.00196078  0.00980392 -0.00196078 -0.00980392\n",
      "  -0.0254902  -0.06470589 -0.12745099 -0.19019608  0.35490197  0.5         0.5\n",
      "   0.3392157  -0.04509804 -0.43333334]\n",
      " [-0.28431374  0.5        -0.0254902  -0.5        -0.49215686 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.49607843\n",
      "  -0.5        -0.44509804  0.32745099  0.5         0.48823529  0.48431373\n",
      "   0.5        -0.20980392]\n",
      " [-0.3509804   0.43725491  0.14705883 -0.5        -0.48823529 -0.49215686\n",
      "  -0.48823529 -0.48823529 -0.48823529 -0.48823529 -0.48431373 -0.48431373\n",
      "  -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373\n",
      "  -0.48431373 -0.47254902 -0.5        -0.28823531  0.42941177  0.5\n",
      "   0.48823529  0.5         0.42941177 -0.31568629]\n",
      " [-0.41764706  0.34705883  0.29607844 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.48823529 -0.48431373 -0.48431373 -0.48431373 -0.48431373\n",
      "  -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373\n",
      "  -0.47254902 -0.5        -0.18627451  0.5         0.5         0.48823529\n",
      "   0.5         0.33529413 -0.39411765]\n",
      " [-0.48431373  0.25686276  0.42156863 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.48039216\n",
      "  -0.5        -0.07647059  0.5         0.48431373  0.48431373  0.5\n",
      "   0.2254902  -0.47254902]\n",
      " [-0.5         0.15490197  0.48823529 -0.4254902  -0.5        -0.49607843\n",
      "  -0.5        -0.48039216 -0.16666667 -0.08039216 -0.07647059 -0.05686275\n",
      "  -0.04117647 -0.03333334 -0.0254902  -0.0372549  -0.06078431 -0.08039216\n",
      "  -0.11176471 -0.17058824 -0.30000001  0.11960784  0.5         0.47647059\n",
      "   0.47647059  0.5         0.11960784 -0.5       ]\n",
      " [-0.5         0.06078431  0.5        -0.31568629 -0.5        -0.48823529\n",
      "  -0.5        -0.44901961  0.31568629  0.5         0.49215686  0.5         0.5\n",
      "   0.5         0.5         0.5         0.5         0.5         0.5         0.5\n",
      "   0.5         0.5         0.5         0.5         0.48431373  0.5\n",
      "   0.00980392 -0.5       ]\n",
      " [-0.5        -0.05294118  0.5        -0.19411765 -0.5        -0.48431373\n",
      "  -0.5        -0.47647059  0.21372549  0.5         0.47254902  0.48823529\n",
      "   0.47647059  0.31568629  0.25294119  0.2372549   0.21372549  0.19411765\n",
      "   0.19803922  0.2254902   0.25686276  0.28823531  0.3392157   0.39803922\n",
      "   0.44117647  0.5        -0.11960784 -0.5       ]\n",
      " [-0.5        -0.17058824  0.5        -0.07254902 -0.5        -0.48039216\n",
      "  -0.5        -0.49607843  0.16666667  0.5         0.48431373  0.5\n",
      "   0.46862745 -0.27254903 -0.5        -0.49215686 -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.49607843 -0.46078432 -0.39411765\n",
      "  -0.33529413 -0.44117647 -0.5       ]\n",
      " [-0.5        -0.28431374  0.5         0.04901961 -0.5        -0.47647059\n",
      "  -0.5        -0.5         0.10784314  0.5         0.48431373  0.5\n",
      "   0.48431373 -0.21372549 -0.5        -0.48431373 -0.49215686 -0.49215686\n",
      "  -0.48823529 -0.49215686 -0.49607843 -0.49607843 -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.39019608  0.49607843  0.15490197 -0.5        -0.48431373\n",
      "  -0.49215686 -0.5         0.05686275  0.5         0.47647059  0.5\n",
      "   0.49607843 -0.1627451  -0.5        -0.47254902 -0.48823529 -0.5        -0.5\n",
      "  -0.49215686 -0.48431373 -0.49607843 -0.49607843 -0.49607843 -0.49215686\n",
      "  -0.49215686 -0.49607843 -0.5       ]\n",
      " [-0.5        -0.49607843  0.43333334  0.25294119 -0.5        -0.5\n",
      "  -0.48823529 -0.5         0.00196078  0.5         0.48039216  0.5\n",
      "   0.49607843 -0.1627451  -0.5        -0.48823529 -0.5        -0.5\n",
      "  -0.45294118 -0.46470588 -0.5        -0.49607843 -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5         0.34705883  0.34313726 -0.43333334 -0.5\n",
      "  -0.48039216 -0.5        -0.06470589  0.5         0.47254902  0.5         0.5\n",
      "   0.23333333  0.05294118  0.13137256  0.16666667  0.25294119  0.41764706\n",
      "   0.2647059  -0.18627451 -0.46862745 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5         0.20980392  0.4254902  -0.36666667 -0.5\n",
      "  -0.48823529 -0.5        -0.44117647 -0.33137256 -0.32352942 -0.29607844\n",
      "  -0.2764706  -0.21372549 -0.18627451 -0.20196079 -0.31176472  0.10392157\n",
      "   0.5         0.49215686  0.5         0.36274511 -0.20980392 -0.5\n",
      "  -0.49607843 -0.5        -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5         0.07647059  0.5        -0.30000001 -0.5\n",
      "  -0.48823529 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.48823529 -0.5         0.09607843  0.5\n",
      "   0.48039216  0.47647059  0.5        -0.00588235 -0.5        -0.49607843\n",
      "  -0.5        -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5        -0.05294118  0.5        -0.23333333 -0.5\n",
      "  -0.48431373 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.48823529 -0.5         0.21764706  0.5\n",
      "   0.49215686  0.48823529  0.5        -0.12745099 -0.5        -0.49215686\n",
      "  -0.5        -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5        -0.16666667  0.5        -0.13529412 -0.5\n",
      "  -0.48431373 -0.5        -0.40196079 -0.1509804  -0.14705883 -0.13921569\n",
      "  -0.13137256 -0.13529412 -0.14313726 -0.14313726 -0.20196079  0.3509804\n",
      "   0.5         0.48431373  0.47647059  0.5        -0.20588236 -0.5\n",
      "  -0.48823529 -0.5        -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5        -0.28039217  0.5        -0.01372549 -0.5\n",
      "  -0.48039216 -0.5        -0.21372549  0.5         0.49607843  0.49607843\n",
      "   0.5         0.5         0.5         0.5         0.5         0.5         0.5\n",
      "   0.5         0.48431373  0.5        -0.26078433 -0.5        -0.48823529\n",
      "  -0.5        -0.5        -0.5       ]\n",
      " [-0.49215686 -0.5        -0.38235295  0.5         0.10392157 -0.5\n",
      "  -0.48431373 -0.5        -0.26078433  0.46078432  0.49215686  0.48039216\n",
      "   0.5         0.44117647  0.30392158  0.29215688  0.29215688  0.30000001\n",
      "   0.31176472  0.31960785  0.33137256  0.33137256 -0.3392157  -0.5\n",
      "  -0.49215686 -0.5        -0.5        -0.5       ]\n",
      " [-0.49607843 -0.5        -0.42156863  0.44117647  0.21764706 -0.5\n",
      "  -0.49215686 -0.5        -0.28039217  0.45294118  0.5         0.48039216\n",
      "   0.5         0.2254902  -0.5        -0.49607843 -0.49607843 -0.5        -0.5\n",
      "  -0.49215686 -0.48823529 -0.48431373 -0.49607843 -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.49607843 -0.5        -0.44509804  0.35490197  0.31568629 -0.48823529\n",
      "  -0.49607843 -0.5        -0.30000001  0.43333334  0.5         0.48039216\n",
      "   0.5         0.22156863 -0.5        -0.49607843 -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.49607843 -0.5        -0.46862745  0.28039217  0.40196079 -0.47647059\n",
      "  -0.5        -0.5        -0.32352942  0.4137255   0.5         0.48039216\n",
      "   0.5         0.23333333 -0.5        -0.49607843 -0.49607843 -0.5\n",
      "  -0.49607843 -0.49607843 -0.49607843 -0.49607843 -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5       ]\n",
      " [-0.49607843 -0.5        -0.48431373  0.21372549  0.46862745 -0.45294118\n",
      "  -0.48823529 -0.5        -0.33137256  0.39803922  0.5         0.48039216\n",
      "   0.5         0.25294119 -0.49215686 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.49607843 -0.5        -0.49215686  0.17843138  0.49215686 -0.46470588\n",
      "  -0.5        -0.5        -0.37450981  0.37058824  0.5         0.48039216\n",
      "   0.5         0.2764706  -0.46470588 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.49607843 -0.5        -0.49215686  0.18235295  0.5         0.08039216\n",
      "   0.06862745  0.09607843  0.17450981  0.44509804  0.49607843  0.47254902\n",
      "   0.5         0.29215688 -0.44117647 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.30000001  0.13137256  0.49607843\n",
      "   0.5         0.5         0.5         0.5         0.5         0.48823529\n",
      "   0.5         0.33137256 -0.40980393 -0.5        -0.49215686 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.49607843 -0.5        -0.37450981\n",
      "   0.05686275  0.24117647  0.20196079  0.17843138  0.14705883  0.11176471\n",
      "   0.1        -0.04117647 -0.45294118 -0.5        -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]]\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(train_labels[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Riportiamo i dati nel formato adatto al processamento: matrix 1-dim + vettore 1-hot encoding\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  # -1 indica che la dimensione iniziale rimane invariata\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  # aggiungo una dimensione a labels\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.14313726  0.37450981  0.22941177  0.20196079  0.18235295  0.15490197\n",
      "  0.13137256  0.10392157  0.07647059  0.0372549  -0.00196078 -0.03333334\n",
      " -0.06470589 -0.08431373 -0.10392157 -0.10392157 -0.08039216 -0.05686275\n",
      " -0.03333334  0.00588235  0.08039216  0.13921569  0.18235295  0.23333333\n",
      " -0.10784314 -0.48039216 -0.5        -0.5        -0.19803922  0.5\n",
      " -0.04117647 -0.34705883 -0.27254903 -0.24509804 -0.21372549 -0.18235295\n",
      " -0.15490197 -0.11568628 -0.07647059 -0.04509804 -0.02156863 -0.00588235\n",
      "  0.00196078  0.00980392 -0.00196078 -0.00980392 -0.0254902  -0.06470589\n",
      " -0.12745099 -0.19019608  0.35490197  0.5         0.5         0.3392157\n",
      " -0.04509804 -0.43333334 -0.28431374  0.5        -0.0254902  -0.5\n",
      " -0.49215686 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.49607843 -0.5        -0.44509804  0.32745099  0.5\n",
      "  0.48823529  0.48431373  0.5        -0.20980392 -0.3509804   0.43725491\n",
      "  0.14705883 -0.5        -0.48823529 -0.49215686 -0.48823529 -0.48823529\n",
      " -0.48823529 -0.48823529 -0.48431373 -0.48431373 -0.48431373 -0.48431373\n",
      " -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.47254902\n",
      " -0.5        -0.28823531  0.42941177  0.5         0.48823529  0.5\n",
      "  0.42941177 -0.31568629 -0.41764706  0.34705883  0.29607844 -0.5\n",
      " -0.49607843 -0.5        -0.5        -0.5        -0.48823529 -0.48431373\n",
      " -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373\n",
      " -0.48431373 -0.48431373 -0.48431373 -0.47254902 -0.5        -0.18627451\n",
      "  0.5         0.5         0.48823529  0.5         0.33529413 -0.39411765\n",
      " -0.48431373  0.25686276  0.42156863 -0.5        -0.49607843 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.48039216\n",
      " -0.5        -0.07647059  0.5         0.48431373  0.48431373  0.5\n",
      "  0.2254902  -0.47254902 -0.5         0.15490197  0.48823529 -0.4254902\n",
      " -0.5        -0.49607843 -0.5        -0.48039216 -0.16666667 -0.08039216\n",
      " -0.07647059 -0.05686275 -0.04117647 -0.03333334 -0.0254902  -0.0372549\n",
      " -0.06078431 -0.08039216 -0.11176471 -0.17058824 -0.30000001  0.11960784\n",
      "  0.5         0.47647059  0.47647059  0.5         0.11960784 -0.5        -0.5\n",
      "  0.06078431  0.5        -0.31568629 -0.5        -0.48823529 -0.5\n",
      " -0.44901961  0.31568629  0.5         0.49215686  0.5         0.5         0.5\n",
      "  0.5         0.5         0.5         0.5         0.5         0.5         0.5\n",
      "  0.5         0.5         0.5         0.48431373  0.5         0.00980392\n",
      " -0.5        -0.5        -0.05294118  0.5        -0.19411765 -0.5\n",
      " -0.48431373 -0.5        -0.47647059  0.21372549  0.5         0.47254902\n",
      "  0.48823529  0.47647059  0.31568629  0.25294119  0.2372549   0.21372549\n",
      "  0.19411765  0.19803922  0.2254902   0.25686276  0.28823531  0.3392157\n",
      "  0.39803922  0.44117647  0.5        -0.11960784 -0.5        -0.5\n",
      " -0.17058824  0.5        -0.07254902 -0.5        -0.48039216 -0.5\n",
      " -0.49607843  0.16666667  0.5         0.48431373  0.5         0.46862745\n",
      " -0.27254903 -0.5        -0.49215686 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.49607843 -0.46078432 -0.39411765 -0.33529413\n",
      " -0.44117647 -0.5        -0.5        -0.28431374  0.5         0.04901961\n",
      " -0.5        -0.47647059 -0.5        -0.5         0.10784314  0.5\n",
      "  0.48431373  0.5         0.48431373 -0.21372549 -0.5        -0.48431373\n",
      " -0.49215686 -0.49215686 -0.48823529 -0.49215686 -0.49607843 -0.49607843\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.39019608  0.49607843  0.15490197 -0.5        -0.48431373 -0.49215686\n",
      " -0.5         0.05686275  0.5         0.47647059  0.5         0.49607843\n",
      " -0.1627451  -0.5        -0.47254902 -0.48823529 -0.5        -0.5\n",
      " -0.49215686 -0.48431373 -0.49607843 -0.49607843 -0.49607843 -0.49215686\n",
      " -0.49215686 -0.49607843 -0.5        -0.5        -0.49607843  0.43333334\n",
      "  0.25294119 -0.5        -0.5        -0.48823529 -0.5         0.00196078\n",
      "  0.5         0.48039216  0.5         0.49607843 -0.1627451  -0.5\n",
      " -0.48823529 -0.5        -0.5        -0.45294118 -0.46470588 -0.5\n",
      " -0.49607843 -0.49607843 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.49215686 -0.5         0.34705883  0.34313726 -0.43333334 -0.5\n",
      " -0.48039216 -0.5        -0.06470589  0.5         0.47254902  0.5         0.5\n",
      "  0.23333333  0.05294118  0.13137256  0.16666667  0.25294119  0.41764706\n",
      "  0.2647059  -0.18627451 -0.46862745 -0.5        -0.49607843 -0.5        -0.5\n",
      " -0.5        -0.5        -0.49215686 -0.5         0.20980392  0.4254902\n",
      " -0.36666667 -0.5        -0.48823529 -0.5        -0.44117647 -0.33137256\n",
      " -0.32352942 -0.29607844 -0.2764706  -0.21372549 -0.18627451 -0.20196079\n",
      " -0.31176472  0.10392157  0.5         0.49215686  0.5         0.36274511\n",
      " -0.20980392 -0.5        -0.49607843 -0.5        -0.5        -0.5\n",
      " -0.49215686 -0.5         0.07647059  0.5        -0.30000001 -0.5\n",
      " -0.48823529 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.48823529 -0.5         0.09607843  0.5\n",
      "  0.48039216  0.47647059  0.5        -0.00588235 -0.5        -0.49607843\n",
      " -0.5        -0.5        -0.5        -0.49215686 -0.5        -0.05294118\n",
      "  0.5        -0.23333333 -0.5        -0.48431373 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.48823529\n",
      " -0.5         0.21764706  0.5         0.49215686  0.48823529  0.5\n",
      " -0.12745099 -0.5        -0.49215686 -0.5        -0.5        -0.5\n",
      " -0.49215686 -0.5        -0.16666667  0.5        -0.13529412 -0.5\n",
      " -0.48431373 -0.5        -0.40196079 -0.1509804  -0.14705883 -0.13921569\n",
      " -0.13137256 -0.13529412 -0.14313726 -0.14313726 -0.20196079  0.3509804\n",
      "  0.5         0.48431373  0.47647059  0.5        -0.20588236 -0.5\n",
      " -0.48823529 -0.5        -0.5        -0.5        -0.49215686 -0.5\n",
      " -0.28039217  0.5        -0.01372549 -0.5        -0.48039216 -0.5\n",
      " -0.21372549  0.5         0.49607843  0.49607843  0.5         0.5         0.5\n",
      "  0.5         0.5         0.5         0.5         0.5         0.48431373\n",
      "  0.5        -0.26078433 -0.5        -0.48823529 -0.5        -0.5        -0.5\n",
      " -0.49215686 -0.5        -0.38235295  0.5         0.10392157 -0.5\n",
      " -0.48431373 -0.5        -0.26078433  0.46078432  0.49215686  0.48039216\n",
      "  0.5         0.44117647  0.30392158  0.29215688  0.29215688  0.30000001\n",
      "  0.31176472  0.31960785  0.33137256  0.33137256 -0.3392157  -0.5\n",
      " -0.49215686 -0.5        -0.5        -0.5        -0.49607843 -0.5\n",
      " -0.42156863  0.44117647  0.21764706 -0.5        -0.49215686 -0.5\n",
      " -0.28039217  0.45294118  0.5         0.48039216  0.5         0.2254902\n",
      " -0.5        -0.49607843 -0.49607843 -0.5        -0.5        -0.49215686\n",
      " -0.48823529 -0.48431373 -0.49607843 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.49607843 -0.5        -0.44509804  0.35490197  0.31568629\n",
      " -0.48823529 -0.49607843 -0.5        -0.30000001  0.43333334  0.5\n",
      "  0.48039216  0.5         0.22156863 -0.5        -0.49607843 -0.49607843\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.49607843 -0.5\n",
      " -0.46862745  0.28039217  0.40196079 -0.47647059 -0.5        -0.5\n",
      " -0.32352942  0.4137255   0.5         0.48039216  0.5         0.23333333\n",
      " -0.5        -0.49607843 -0.49607843 -0.5        -0.49607843 -0.49607843\n",
      " -0.49607843 -0.49607843 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.49607843 -0.5        -0.48431373  0.21372549  0.46862745\n",
      " -0.45294118 -0.48823529 -0.5        -0.33137256  0.39803922  0.5\n",
      "  0.48039216  0.5         0.25294119 -0.49215686 -0.5        -0.49607843\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.49607843 -0.5\n",
      " -0.49215686  0.17843138  0.49215686 -0.46470588 -0.5        -0.5\n",
      " -0.37450981  0.37058824  0.5         0.48039216  0.5         0.2764706\n",
      " -0.46470588 -0.5        -0.49607843 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.49607843 -0.5        -0.49215686  0.18235295  0.5         0.08039216\n",
      "  0.06862745  0.09607843  0.17450981  0.44509804  0.49607843  0.47254902\n",
      "  0.5         0.29215688 -0.44117647 -0.5        -0.49607843 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.30000001\n",
      "  0.13137256  0.49607843  0.5         0.5         0.5         0.5         0.5\n",
      "  0.48823529  0.5         0.33137256 -0.40980393 -0.5        -0.49215686\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.49607843 -0.5        -0.37450981  0.05686275  0.24117647  0.20196079\n",
      "  0.17843138  0.14705883  0.11176471  0.1        -0.04117647 -0.45294118\n",
      " -0.5        -0.49607843 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(train_labels[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PROBLEMA #1\n",
    "# Implementare una logistic regression multinomiale con discesa del gradiente \n",
    "# con Tensorflow (TF) come classificatore per notMNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nh+EgSeowHCRJHYaDJKnj/wG1jsZ7sdjKWwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112aabb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# La matrice dei pensi W viene spesso inizializzata con una variabile casuale con distribuzione normale,\n",
    "# dove i valori maggiori di 2 x std_dev sono rimossi.\n",
    "# weights = tf.Variable( tf.truncated_normal(...))\n",
    "\n",
    "# Questo permette di ignorare valori troppo grandi o piccoli che possono influenzare negativamente l'apprendimento.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "n = 500000\n",
    "A = tf.truncated_normal((n,))\n",
    "B = tf.random_normal((n,))\n",
    "with tf.Session() as sess:\n",
    "    a, b = sess.run([A, B])\n",
    "\n",
    "plt.hist(a, 100, (-4.2, 4.2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112aa6b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(b, 100, (-4.2, 4.2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prendiamo un subset per limitare il tempo per l'addetramento, diminuiscilo se occorre\n",
    "train_subset = 10000\n",
    "\n",
    "# Su Tensorflow ogni elemento - input, variabili ed elaborazioni - è descritto mediante un grafo, o dataflow graph.\n",
    "# Gli oggetti tf.Operation rappresentano unità di computazione;\n",
    "# Gli oggetti tf.Tensor rappresentano unità di dati (tensori) che sono usati come input e output per gli oggetti Operation.\n",
    "\n",
    "# In TF un grafo tf.Graph contiene due tipi di informazione:\n",
    "# La struttura: nodi e archi che rappresentano le operazioni \n",
    "# Le collections: insiemi di metadati (inseriti con tf.add_to_collection) nella forma <chiave,lista di objects); si può ispezionare con tf.get_collection.\n",
    "\n",
    "# TF usa questa struttura per salvare variabili e altre informazioni del grafo.\n",
    "\n",
    "# Un oggetto Graph di default è sempre prensente e accedibile chiamando tf.get_default_graph. \n",
    "\n",
    "# Un approccio alternativo per usare i grafo di Tensorflow consiste nel context manager tf.Graph.as_default, \n",
    "# che sostituisce il grafo di default per tutta l'esistenza del contesto in esame.\n",
    "graph = tf.Graph()\n",
    "\n",
    "# Costruisco un grafo di computazione con Tensorflow\n",
    "with graph.as_default():\n",
    "\n",
    "  # Creo tensori costanti per i seguenti set: trainig, test e validation\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  # per assegnare un nome alla variabile possiamo usare il secondo parametro, e.g., tf.constant(0, name=\"c\") \n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Le variabili mantengono lo stato durante le elaborazioni. Sono anch'esse tensori.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  # Il vettore di bias b è inizializzato a 0.\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Calcolo Wx + b\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "\n",
    "  # La funzione softmax_cross_entropy_with_logits valuta la funzione di loss\n",
    "  # per mezzo della cross-entropy loss con l'output corretto (tf_train_labels)\n",
    "  # Mentre reduce_mean valuta semplicemente la media dei valori del tensore.\n",
    "  # loss indica una operazione TF.\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Instanzio un algoritmo di discesa del gradiente con learning rate = 0.5 (alfa nelle slide di richiami sulle reti neurali.)\n",
    "  # La funzione minimize è composta di 2 elaborazioni: compute_gradients e apply_gradients.\n",
    "  # La prima ricava i gradienti, la seconda aggiorna la matrice dei pesi di conseguenza.\n",
    "  # optimizer indica una operazione TF.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "  # calcolo softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), dim) per i sets: \n",
    "  # training, validation e test.\n",
    "  # N.B.: i set valid e test sono usati solo per la valutazione, non c'è backprop\n",
    "  # N.B.(2): ci servono per valutare l'accuratezza, l'apprendimento l'abbiamo già fatto.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  logits = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "  valid_prediction = tf.nn.softmax(logits) \n",
    "  logits = tf.matmul(tf_test_dataset, weights) + biases\n",
    "  test_prediction = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 15.487743\n",
      "Training accuracy: 10.8%\n",
      "Validation accuracy: 13.6%\n",
      "Loss at step 100: 2.307485\n",
      "Training accuracy: 71.7%\n",
      "Validation accuracy: 71.3%\n",
      "Loss at step 200: 1.875020\n",
      "Training accuracy: 74.8%\n",
      "Validation accuracy: 73.8%\n",
      "Loss at step 300: 1.632705\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 74.5%\n",
      "Loss at step 400: 1.466148\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 75.0%\n",
      "Loss at step 500: 1.341997\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 75.3%\n",
      "Loss at step 600: 1.244797\n",
      "Training accuracy: 78.4%\n",
      "Validation accuracy: 75.5%\n",
      "Loss at step 700: 1.165597\n",
      "Training accuracy: 79.0%\n",
      "Validation accuracy: 75.7%\n",
      "Loss at step 800: 1.099145\n",
      "Training accuracy: 79.4%\n",
      "Validation accuracy: 75.8%\n",
      "Test accuracy: 82.2%\n"
     ]
    }
   ],
   "source": [
    "# numero cicli di elaborazione\n",
    "num_steps = 801\n",
    "\n",
    "# Definisco l'accuratezza come somma del numero di predizioni corrette \n",
    "# normalizzato sul numero di predizioni totali.\n",
    "# La uso per fare statistiche durante il funzionamento.\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "# TF usa tf.Session per rappresentare una connesione tra il programa e il runtime C++.\n",
    "# Serve per creare un ambiente in cui lanciare le operazioni definite nel grafo.\n",
    "# Poichè la classe alloca risorse fisiche, solitamente si usa come context manager (dentro un blocco with),\n",
    "# che le libera automaticamente al termine del blocco, cioè lancia session.close() al termine della esecuzione.\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # Istanzia e lancia una operazione per l'inizializzazione delle variabili globali del grafo\n",
    "  # cioè: weights e biases. Va eseguita solo una volta.\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "      \n",
    "  for step in range(num_steps):\n",
    "    # Eseguo le operazioni nel grafo.\n",
    "    # Le operazioni e i tensori da valutare sono definiti nel primo parametro, un NumPy array.\n",
    "    # La lista indica le foglie grafo.\n",
    "    # Il valore di ritorno ha lo stesso tipo dell'input, cioè un array, \n",
    "    # dove le foglie sono sostituite con il corrispondente valore calcolato da TF.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    # ogni tanto stampo statistiche\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Se invoco eval() su valid_prediction, sto calcolando l'operazione sui \n",
    "      # pesi e bias correnti.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  # Al termine stamo l'accuracy sul test set.\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# L'accuratezza del training aumenta mentre la validation rimane costante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PROBLEMA #2\n",
    "# Prova a modificare il codice precedente impiegando un Stochastic gradient descent.\n",
    "# Quanto tempo impiega ora per terminare l'elaborazione?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  6.]\n"
     ]
    }
   ],
   "source": [
    "# Durante l'elaborazone batch l'algoritmo elabora solo un sottoinsieme di dati alla volta.\n",
    "# L'elaborazione è ripetuta, perciò conviene scrivere il codice \n",
    "# senza gestire la creazione dei dati direttamente.\n",
    "# In TF un placeholder è una variabile che assumera i valori a tempo di esecuzione.\n",
    "# Possiamo costruire il grafo delle operazioni senza il bisogno di conoscere i dati.\n",
    "\n",
    "# Nel seguente codice creiamo una operazione (y) di moltiplicazione * 2 senza sapere i valori.\n",
    "# Ora la possiamo eseguire all'interno di una sessione. Per valutarla occorre fornire (feed)\n",
    "# i valori per x. \n",
    "# None significa che non poniamo vincoli sulla dimensione.\n",
    "x = tf.placeholder(tf.float32, shape=None)\n",
    "y = x * 2\n",
    "\n",
    "# TF supporta tipi di variabili simili a NumPy (es. float32, float64, int32, int64)\n",
    "# https://docs.scipy.org/doc/numpy-1.13.0/user/basics.types.html\n",
    "\n",
    "with tf.Session() as session:\n",
    "    result = session.run(y, feed_dict={x: [1, 2, 3]})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N.B. Fuori dallo scope session non possiamo stampare il valore dei tensori \n",
    "# durante l'elaborazione.\n",
    "\n",
    "# x = tf.placeholder(\"float\", None)\n",
    "# y = x * 2\n",
    "# print(x) \n",
    "\n",
    "# Output: \"Tensor(\"Placeholder_11:0\", dtype=float32)\" \n",
    "# Stampa solo il tipo e non il valore di x.\n",
    "# In alternativa usare https://www.tensorflow.org/api_docs/python/tf/InteractiveSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.   4.   6.]\n",
      " [  8.  10.  12.]]\n"
     ]
    }
   ],
   "source": [
    "# Possiamo dare in input anche strutture più complesse indicando il formato dei dati con shape.\n",
    "# Es. un qualsiasi numero di righe, ma il numero di colonne pari a 3\n",
    "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = x * 2\n",
    "\n",
    "with tf.Session() as session:\n",
    "    x_data = [[1, 2, 3],\n",
    "              [4, 5, 6],]\n",
    "    result = session.run(y, feed_dict={x: x_data})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Una immagine a colori (RGB) in formato raw può avere una rappresentazione matriciale\n",
    "#image = tf.placeholder(\"uint8\", shape=[None, None, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Una operazione placeholder viene usata per alimentare il grafo.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "\n",
    "  # Il resto è uguale al precedente esempio.\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  logits = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "  valid_prediction = tf.nn.softmax(logits) \n",
    "  logits = tf.matmul(tf_test_dataset, weights) + biases\n",
    "  test_prediction = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 20.357498\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 12.7%\n",
      "Minibatch loss at step 500: 1.414466\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 1000: 1.108463\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 1500: 1.075534\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 2000: 1.007727\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 2500: 1.123234\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 3000: 1.145665\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 79.3%\n",
      "Test accuracy: 85.8%\n"
     ]
    }
   ],
   "source": [
    "# Se impiego minibatch potenzialmente ho più varianza nell'apprendimento ad ogni ciclo.\n",
    "# Sono costretto ad aumentare gli step.\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "    \n",
    "  for step in range(num_steps):\n",
    "    # Definisco un offset nel trainig set\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    \n",
    "    # Estraggo ilminibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "    # Dizionario {chiave_placeholder : valore, ...}\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    \n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# L'accuratezza sul minibatch diminuisce mentre quella sulla validation aumenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PROBLEMA #3\n",
    "# Usando l'help online di TF prova a creare una rete neurale con 1-hidden layer\n",
    "# con attivazione RELU e 1024 nodi nascosti.\n",
    "\n",
    "# N.B. la funzione tf.nn.relu() restituisce un tensore che calcola la RELU sul tensore di input.\n",
    "# L'output ha la stessa dimensione dell'input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nodi del hidden layer\n",
    "hidden_nodes= 1024\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Avendo un layer in più ho due coppie <W,B>\n",
    "    # N.B. W1 e B1 hanno dimensioni <#input-feature-vector,#hidden-nodes>, <#hidden-nodes>\n",
    "    # mentre W2 <#hidden-nodes,num_labels>, <num_labels>\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "    biases_1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    \n",
    "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "\n",
    "    # Ora la loss function è definita sullo layer di output\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits \\\n",
    "                          (labels=tf_train_labels, logits=logits_2))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits_2)\n",
    "    # Seguo la stessa pipeline per valid e test set\n",
    "    logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    valid_prediction = tf.nn.softmax(logits_2) \n",
    "    \n",
    "    logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    test_prediction = tf.nn.softmax(logits_2)                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 349.692200\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 28.5%\n",
      "Minibatch loss at step 500: 16.494976\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 1000: 14.900915\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 1500: 6.343303\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 2000: 4.461616\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 2500: 3.223941\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 3000: 4.156029\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.7%\n",
      "Test accuracy: 88.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "    \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    \n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    \n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# L'accuratezza sul minibatch diminuisce mentre quella sulla validation aumenta molto più rapidamente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
