{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# suppongo di aver scaricato notMNIST_large.tar.gz e notMNIST_small.tar.gz\n",
    "# decompressi in 2 directory notMNIST_large e notMNIST_small\n",
    "train_folder = \"../datasets/notMNIST_large/\"\n",
    "test_folder = \"../datasets/notMNIST_small/\"\n",
    "num_classes = 10 # notMNIST labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 28  # dimensione immagine dataset: 28x28 pixel\n",
    "pixel_depth = 255.0 # scale di grigio\n",
    "\n",
    "def load_images(folder, min_num_images):\n",
    "    print (\"load from %s\" % folder)\n",
    "    image_files = os.listdir(folder)\n",
    "    # creo una matrice NUM_FILES x 28 x 28 di float per memorizzare il dataset\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "    num_images = 0\n",
    "    for image in image_files:\n",
    "        image_file = os.path.join(folder, image)\n",
    "        # posso avere errori nella lettura delle immagini\n",
    "        try:      \n",
    "            # leggo una immagine e la memorizzo come matrice\n",
    "            # opero anche la normalizzazione mean = 0 e standard deviation ~0.5\n",
    "            image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "            if image_data.shape != (image_size, image_size):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "            # accodo al dataset l'immagine\n",
    "            dataset[num_images, :, :] = image_data\n",
    "            num_images = num_images + 1\n",
    "        except IOError as e:\n",
    "            print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "            \n",
    "    # cropping della matrice conteggiando le immagini effettivamente lette\n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('Many fewer images than expected: %d < %d' % (num_images, min_num_images))\n",
    "    \n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    print('Mean:', np.mean(dataset))\n",
    "    print('Standard deviation:', np.std(dataset))\n",
    "    return dataset\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# recupero tutte le sottodirectory del parametro directory\n",
    "def list_subdirs(directory):\n",
    "    data_folders = [os.path.join(directory, d) for d in sorted(os.listdir(directory)) if os.path.isdir(os.path.join(directory, d))]\n",
    "    if len(data_folders) != num_classes:\n",
    "        raise Exception('Expected %d folders, one per class. Found %d instead.' % (num_classes, len(data_folders)))\n",
    "    # debug: print(data_folders)\n",
    "    return data_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PROBLEMA #1: FASE DI EXPLORATION \n",
    "# Ho tanti dati, è opportuno dare un'occhiata a come sono fatti\n",
    "# prendi dei sample e visualizzali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_subfolders = list_subdirs(train_folder)\n",
    "for folder in train_subfolders:\n",
    "    fn_images = os.listdir(folder)\n",
    "    for file in fn_images[:5]:\n",
    "        path = folder + os.sep + file\n",
    "        print (path)\n",
    "        display(Image(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# carica le immagini presenti nel data_folders e serializza la matrice corrispondente su un file con estensione .pickle\n",
    "def serialize_folder_images(data_folders, min_num_images_per_class):\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "\n",
    "        set_filename = folder + '.pickle'\n",
    "        if os.path.isfile(set_filename):\n",
    "            print ('pickle file %s exists, so I skip it', set_filename)\n",
    "            continue\n",
    "        dataset_names.append(set_filename)\n",
    "        dataset = load_images(folder, min_num_images_per_class)\n",
    "        try:\n",
    "            with open(set_filename, 'wb') as f:\n",
    "                pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "    return dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_subfolders = list_subdirs(train_folder)\n",
    "test_subfolders = list_subdirs(test_folder)\n",
    "train_datasets = serialize_folder_images(train_subfolders, 45000)\n",
    "test_datasets = serialize_folder_images(test_subfolders, 1800)\n",
    "print (\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PROBLEMA #2\n",
    "# Ora prova a visualizzare i dati a partire dagli oggetti ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prendo il fn del pickle relativo al label A\n",
    "pickle_file = train_datasets[0]  \n",
    "\n",
    "with open(pickle_file, 'rb') as f:        \n",
    "    # unpickle\n",
    "    letter_set = pickle.load(f)  \n",
    "    # prendo in indice a caso\n",
    "    sample_idx = np.random.randint(len(letter_set))    \n",
    "    # estraggo la matrice relativa all'indice\n",
    "    sample_image = letter_set[sample_idx, :, :]  \n",
    "    plt.figure()\n",
    "    \n",
    "    plt.imshow(sample_image)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PROBLEMA #3 \n",
    "# Nella classificazione multiclass è opportuno avere un dataset bilanciato\n",
    "# verifica che il numero di file per label sia più o meno lo stesso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pickle_file in train_datasets:\n",
    "    with open(pickle_file, 'rb') as f:        \n",
    "        # unpickle\n",
    "        letter_set = pickle.load(f)  \n",
    "        print(\"pickle file \", pickle_file, \" contains \", len(letter_set), \" samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# crea una matrice di double per il dataset, e un array di int per i label, di dimensioni nb_rows x nb_rows e nb_rows rispettivamente\n",
    "def make_arrays(nb_rows, img_size):\n",
    "    if nb_rows:\n",
    "        dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "        labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "    else:\n",
    "        dataset, labels = None, None\n",
    "    return dataset, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fa il merge dei pickle files \n",
    "# il merge ottenuto lo suddivide in due datasets: uno per il training e uno per la validazione\n",
    "# il primo ha dimensione train_size e il secondo valid_size\n",
    "def merge_pickles(pickle_files, train_size, valid_size=0):\n",
    "\tnum_classes = len(pickle_files)\n",
    "\tvalid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "\ttrain_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "    # numero di istanze da considerare (fisso) per label \n",
    "\tvalid_size_per_class = valid_size // num_classes\n",
    "\ttrain_size_per_class = train_size // num_classes\n",
    "\t\t\n",
    "\tstart_v, start_t = 0, 0\n",
    "\tend_v, end_t = valid_size_per_class, train_size_per_class\n",
    "\tend_l = valid_size_per_class+train_size_per_class\n",
    "    # itera sui pickle files insieme a un contatore (label)\n",
    "\tfor label, pickle_file in enumerate(pickle_files):\t\t\t \n",
    "\t\ttry:\n",
    "\t\t\twith open(pickle_file, 'rb') as f:\n",
    "\t\t\t\tletter_set = pickle.load(f)\n",
    "\t\t\t\t# faccio uno shuffle del dataset, perchè? \n",
    "\t\t\t\tnp.random.shuffle(letter_set)\n",
    "\t\t\t\tif valid_dataset is not None:\n",
    "                    # di tutto il pickle prendo solo valid_size_per_class istanze\n",
    "\t\t\t\t\tvalid_letter = letter_set[:valid_size_per_class, :, :]\n",
    "\t\t\t\t\tvalid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "\t\t\t\t\tvalid_labels[start_v:end_v] = label\n",
    "\t\t\t\t\tstart_v += valid_size_per_class\n",
    "\t\t\t\t\tend_v += valid_size_per_class\n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\ttrain_letters = letter_set[valid_size_per_class:end_l, :, :]\n",
    "\t\t\t\ttrain_dataset[start_t:end_t, :, :] = train_letters\n",
    "\t\t\t\ttrain_labels[start_t:end_t] = label\n",
    "\t\t\t\tstart_t += train_size_per_class\n",
    "\t\t\t\tend_t += train_size_per_class\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint('Unable to process data from', pickle_file, ':', e)\n",
    "\t\t\traise\n",
    "\t\t\n",
    "\treturn valid_dataset, valid_labels, train_dataset, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# definiamo le dimensioni dei dataset per il training, la validazione e il test\n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_pickles(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_pickles(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dato un dataset e i corrispondenti labels, fa un nuovo shuffle\n",
    "def randomize(dataset, labels):\n",
    "    # restituisce un array contenente le permutazioni del numero dato in input\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# salviamo i dataset su un singolo file\n",
    "pickle_file = \"../datasets/notMNIST.pickle\"\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'train_dataset': train_dataset,\n",
    "        'train_labels': train_labels,\n",
    "        'valid_dataset': valid_dataset,\n",
    "        'valid_labels': valid_labels,\n",
    "        'test_dataset': test_dataset,\n",
    "        'test_labels': test_labels,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PROBLEMA #4\n",
    "# valutare la possibilità di overlap (stesse immagini) tra i datasets di training, validation e test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_overlaps(images1, images2):\n",
    "    images1.flags.writeable=False\n",
    "    images2.flags.writeable=False\n",
    "    # Python v2\n",
    "    #hash1 = set([hash(image1.data) for image1 in images1])\n",
    "    #hash2 = set([hash(image2.data) for image2 in images2])\n",
    "    # Python v3\n",
    "    hash1 = set([hash(image1.tobytes()) for image1 in images1])\n",
    "    hash2 = set([hash(image2.tobytes()) for image2 in images2])\n",
    "    all_overlaps = set.intersection(hash1, hash2)\n",
    "    return all_overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = check_overlaps(train_dataset, test_dataset)    \n",
    "print('Number of overlaps between training and test sets: ', len(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PROBLEMA #5\n",
    "# Prova ad addestrare un classificatore della libreria ML sklearn \n",
    "# Magari trovi una soluzione soddisfacente\n",
    "# Ti consiglio di partire da poche istanze di training, es. 50, 100, 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PROBLEMA #6\n",
    "# Come mai il data shuffle è così importante?\n",
    "# In quali circostanze l'assenza di shuffle può diminuire l'accuratezza? \n",
    "# In quali altre circostanze lo shuffle può essere inutile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se vuoi approfondisci l'argomento studiando l'interleaving learning https://arxiv.org/abs/1611.05607"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
